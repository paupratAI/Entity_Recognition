{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pràctica 3 - PLH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realitzada pels alumnes Lluc Furriols i Pau Prat Moreno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os \\nf = open(\"/dev/null\", \"w\")\\nos.dup2(f.fileno(), 2)\\nf.close()\\n\\nimport nltk\\nimport ssl\\n\\ntry:\\n    _create_unverified_https_context = ssl._create_unverified_context\\nexcept AttributeError:\\n    pass\\nelse:\\n    ssl._create_default_https_context = _create_unverified_https_context\\n\\nnltk.download()\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import os \n",
    "f = open(\"/dev/null\", \"w\")\n",
    "os.dup2(f.fileno(), 2)\n",
    "f.close()\n",
    "\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importació de llibreries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt', quiet=True) # Tokenitzador\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True) # Etiquetador POS\n",
    "nltk.download('maxent_ne_chunker', quiet=True) # Etiquetador Entitats Anomenades\n",
    "nltk.download('words', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carreguem les dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2002 to\n",
      "[nltk_data]     C:\\Users\\llucfurriols\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2002 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('conll2002')\n",
    "from nltk.corpus import conll2002\n",
    "\n",
    "train_esp = conll2002.iob_sents('esp.train') # Train, \n",
    "val_esp = conll2002.iob_sents('esp.testa') # Val\n",
    "test_esp = conll2002.iob_sents('esp.testb') # Test\n",
    "\n",
    "train_ned = conll2002.iob_sents('ned.train') # Train\n",
    "val_ned = conll2002.iob_sents('ned.testa') # Val\n",
    "test_ned = conll2002.iob_sents('ned.testb') # Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Melbourne', 'NP', 'B-LOC'), ('(', 'Fpa', 'O'), ('Australia', 'NP', 'B-LOC'), (')', 'Fpt', 'O'), (',', 'Fc', 'O'), ('25', 'Z', 'O'), ('may', 'NC', 'O'), ('(', 'Fpa', 'O'), ('EFE', 'NC', 'B-ORG'), (')', 'Fpt', 'O'), ('.', 'Fp', 'O')], [('-', 'Fg', 'O')], ...]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_esp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token(sequence):\n",
    "    \"\"\"\n",
    "    Retorna una llista de tokens.\n",
    "    \"\"\"\n",
    "    return [[(token) for token, pos, entity in sentence] for sentence in sequence]\n",
    "\n",
    "def get_token_entity(sequence):\n",
    "    \"\"\"\n",
    "    Retorna una llista de tokens i les seves entitats (BIOES...).\n",
    "    \"\"\"\n",
    "    return [[(token, entity) for token, pos, entity in sentence] for sentence in sequence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First execution with no modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quina forma tenen les nostres dades d'entrenament:  [('Melbourne', 'B-LOC'), ('(', 'O'), ('Australia', 'B-LOC'), (')', 'O'), (',', 'O'), ('25', 'O'), ('may', 'O'), ('(', 'O'), ('EFE', 'B-ORG'), (')', 'O'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import CRFTagger\n",
    "import pycrfsuite\n",
    "\n",
    "ct = CRFTagger(feature_func=None)\n",
    "#train and test sets without the postag\n",
    "train_esp_first = get_token_entity(train_esp)\n",
    "test_esp_first = get_token_entity(test_esp)\n",
    "print(\"Quina forma tenen les nostres dades d'entrenament: \",train_esp_first[0])\n",
    "\n",
    "ct.train(train_esp_first, 'model.crf.tagger')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Probar el model en el conjunt de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('La', 'B-LOC'), ('Coruña', 'I-LOC'), (',', 'O'), ('23', 'O'), ('may', 'O'), ('(', 'O'), ('EFECOM', 'B-ORG'), (')', 'O'), ('.', 'O')], [('-', 'O')]]\n"
     ]
    }
   ],
   "source": [
    "# Predir les entitats del conjunt de test\n",
    "y_pred = ct.tag_sents(get_token(test_esp))\n",
    "print(y_pred[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('La', 'B-LOC'), ('Coruña', 'I-LOC'), (',', 'O'), ('23', 'O'), ('may', 'O'), ('(', 'O'), ('EFECOM', 'B-ORG'), (')', 'O'), ('.', 'O')], [('-', 'O')]]\n"
     ]
    }
   ],
   "source": [
    "# Les entitats reals del conjunt de test\n",
    "y_real = get_token_entity(test_esp)\n",
    "print(y_real[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.710312\n",
      "Recall: 0.710312\n",
      "F1 Score: 0.710312\n"
     ]
    }
   ],
   "source": [
    "def extract_entities(tagged_words):\n",
    "    entities = []\n",
    "    current_entity = []\n",
    "    current_type = None\n",
    "    current_start_index = None  # Afegim una variable per guardar l'índex d'inici\n",
    "\n",
    "    for index, (word, tag) in enumerate(tagged_words):\n",
    "        if tag.startswith('B-'):  # Començament d'una nova entitat\n",
    "            if current_entity:  # Si hi havia una entitat en construcció, l'afegim abans de començar la nova\n",
    "                entities.append((current_start_index, index - 1, current_type))\n",
    "            current_entity = [word]  # Comencem una nova entitat\n",
    "            current_start_index = index  # Guardem l'índex d'inici de l'entitat actual\n",
    "            current_type = tag[2:]  # Guardem el tipus d'entitat sense el prefix B-\n",
    "        elif tag.startswith('I-') and current_type == tag[2:]:  # Continuació de la mateixa entitat\n",
    "            current_entity.append(word)\n",
    "        else:  # Si no és una continuació de la mateixa entitat o és 'O'\n",
    "            if current_entity:  # Finalitzem l'entitat actual si n'hi ha una\n",
    "                entities.append((current_start_index, index - 1, current_type))\n",
    "                current_entity = []\n",
    "                current_type = None\n",
    "            if tag == 'O':\n",
    "                continue\n",
    "            else:  # Codificació IO o canvi d'entitat amb I-\n",
    "                current_entity = [word]\n",
    "                current_start_index = index\n",
    "                current_type = tag[2:]  # Possible en cas de codificació IO\n",
    "\n",
    "    # Assegurar-se d'afegir l'última entitat si la llista no acaba en 'O'\n",
    "    if current_entity:\n",
    "        entities.append((current_start_index, index, current_type))\n",
    "\n",
    "    return entities\n",
    "\n",
    "\n",
    "def evaluate_entities(y_test, y_pred, print_errors=False):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a named entity recognition model.\n",
    "\n",
    "    This function calculates the precision, recall, and F1 score of the model's predictions. It only evaluates predictions in terms of entities, not individual tokens.\n",
    "    It also optionally prints the sentences where the model made errors.\n",
    "\n",
    "    Parameters:\n",
    "    y_test (list): The true labels for the test data. Each element is a list of tuples, where each tuple contains a token and its true label.\n",
    "    y_pred (list): The predicted labels for the test data. Each element is a list of tuples, where each tuple contains a token and its predicted label.\n",
    "    print_errors (bool, optional): Whether to print the sentences where the model made errors. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    precision (float): The precision of the model's predictions.\n",
    "    recall (float): The recall of the model's predictions.\n",
    "    f1_score (float): The F1 score of the model's predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    total_entities = 0\n",
    "    correct_entities = 0\n",
    "\n",
    "    for sent_test, sent_pred in zip(y_test, y_pred):\n",
    "        true_entities = extract_entities(sent_test)\n",
    "        pred_entities = extract_entities(sent_pred)\n",
    "        \n",
    "        # Comptar entitats reals\n",
    "        entities_sentence = len(true_entities)\n",
    "        # Entitats correctament predites\n",
    "        entities_predicted = len([e for e in true_entities if e in pred_entities])\n",
    "\n",
    "        # Portem el compte de totes les entitats\n",
    "        total_entities += entities_sentence\n",
    "        # Portem el compte de les entitats correctament predites\n",
    "        correct_entities += entities_predicted\n",
    "\n",
    "        if print_errors:\n",
    "            if entities_sentence != entities_predicted:\n",
    "                print('Real sentence:', sent_test)\n",
    "                print('Predicted:', sent_pred)\n",
    "                print('Difference in:', (set(true_entities) - set(pred_entities)))\n",
    "                print()\n",
    "\n",
    "        \n",
    "    if total_entities == 0:\n",
    "        return 0, 0, 0, 0\n",
    "\n",
    "    precision = correct_entities / total_entities\n",
    "    recall = correct_entities / total_entities\n",
    "    f1_score = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    return precision, recall, f1_score\n",
    "\n",
    "\n",
    "precision, recall, f1_score = evaluate_entities(y_real, y_pred, print_errors=False)\n",
    "print(f'Precision: {precision:.6f}')\n",
    "print(f'Recall: {recall:.6f}')\n",
    "print(f'F1 Score: {f1_score:.6f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canviar feature functions i codificacio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bias': 1.0, 'has_capitalization': True, 'has_digit': False, 'has_punctuation': False, 'all_caps': False, 'is_capitalized': True, 'prefix': 'Bar', 'suffix': 'ona'}\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "class FeatureGetter:\n",
    "    \"\"\"\n",
    "    Aquesta classe s'utilitza per obtenir diferents característiques d'un token de text.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def has_capitalization(self, token):\n",
    "        return any(char.isupper() for char in token)\n",
    "\n",
    "    def has_digit(self, token):\n",
    "        return any(char.isdigit() for char in token)\n",
    "\n",
    "    def has_punctuation(self, token):\n",
    "        return any(char in string.punctuation for char in token)\n",
    "\n",
    "    def get_prefix(self, token, n=3):\n",
    "        return token[:n] if len(token) > n else token\n",
    "\n",
    "    def get_suffix(self, token, n=3):\n",
    "        return token[-n:] if len(token) > n else token\n",
    "\n",
    "    def all_caps(self, token):\n",
    "        return token.isupper()\n",
    "\n",
    "    def is_capitalized(self, token):\n",
    "        return token[0].isupper()\n",
    "\n",
    "    def get_features(self, tokens, index, add_prefix_suffix=True):\n",
    "        token = tokens[index]\n",
    "        token = str(token)\n",
    "        features = {\n",
    "            'bias': 1.0,\n",
    "            'has_capitalization': self.has_capitalization(token),\n",
    "            'has_digit': self.has_digit(token),\n",
    "            'has_punctuation': self.has_punctuation(token),\n",
    "            'all_caps': self.all_caps(token),\n",
    "            'is_capitalized': self.is_capitalized(token),\n",
    "        }\n",
    "\n",
    "        if add_prefix_suffix:\n",
    "            features['prefix'] = self.get_prefix(token)\n",
    "            features['suffix'] = self.get_suffix(token)\n",
    "\n",
    "        return features\n",
    "\n",
    "# Exemple d'ús:\n",
    "feature_getter = FeatureGetter()\n",
    "tokens = ['Barcelona', 'is', 'beautiful']\n",
    "token_features = feature_getter.get_features(tokens, 0)\n",
    "print(token_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificació BIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 4, 'PER'), (13, 14, 'ORG'), (18, 18, 'LOC'), (20, 20, 'LOC'), (59, 59, 'MISC')]\n"
     ]
    }
   ],
   "source": [
    "def extract_entities(tagged_words, encoding='BIO'):\n",
    "    \"\"\"\n",
    "    Extreu les entitats d'una llista de paraules etiquetades segons l'encoding especificat.\n",
    "    \n",
    "    Arguments:\n",
    "        tagged_words: una llista de tuples (word, tag), on 'word' és una paraula del text i 'tag' és la seva etiqueta (BIO/BIOE/BIOW/IO).\n",
    "        encoding: el tipus de codificació utilitzat per les etiquetes ('BIO', 'BIOW', 'BIOE', 'IO').\n",
    "        \n",
    "    Retorna:\n",
    "        Una llista de tuples (start_index, end_index, entity_type) que representen les entitats trobades.\n",
    "        'start_index' i 'end_index' són els índexs on comença i acaba l'entitat en la llista de paraules, i 'entity_type' és el tipus d'entitat.\n",
    "    \"\"\"\n",
    "\n",
    "    entities = []  # Llista on guardarem les entitats trobades\n",
    "    current_entity = []  # Guarda les paraules de l'entitat actual\n",
    "    current_type = None  # Tipus de l'entitat actual\n",
    "    current_start_index = None  # Índex d'inici de l'entitat actual\n",
    "\n",
    "    for index, (word, tag) in enumerate(tagged_words):\n",
    "        tag_type = None if tag == 'O' else tag[2:]\n",
    "\n",
    "        if tag == 'O':\n",
    "            if current_entity:\n",
    "                entities.append((current_start_index, index - 1, current_type))\n",
    "                current_entity = []\n",
    "                current_type = None\n",
    "            continue\n",
    "\n",
    "        if encoding == 'IO':\n",
    "            if tag_type != current_type:\n",
    "                if current_entity:\n",
    "                    entities.append((current_start_index, index - 1, current_type))\n",
    "                current_entity = [word]\n",
    "                current_start_index = index\n",
    "                current_type = tag_type\n",
    "            else:\n",
    "                current_entity.append(word)\n",
    "        else:\n",
    "            tag_prefix = tag[:1]\n",
    "            if tag_prefix in ['B', 'W']:  # Començament d'una nova entitat o entitat de paraula única\n",
    "                if current_entity:\n",
    "                    entities.append((current_start_index, index - 1, current_type))\n",
    "                current_entity = [word]\n",
    "                current_start_index = index\n",
    "                current_type = tag_type\n",
    "                if tag_prefix == 'W':  # Si és una entitat de paraula única, la tanquem immediatament\n",
    "                    entities.append((current_start_index, index, current_type))\n",
    "                    current_entity = []\n",
    "                    current_type = None\n",
    "            elif tag_prefix == 'I' and current_type == tag_type:\n",
    "                current_entity.append(word)\n",
    "            elif encoding == 'BIOE' and tag_prefix == 'E' and current_type == tag_type:\n",
    "                current_entity.append(word)\n",
    "                entities.append((current_start_index, index, current_type))\n",
    "                current_entity = []\n",
    "                current_type = None\n",
    "            else:\n",
    "                if current_entity:\n",
    "                    entities.append((current_start_index, index - 1, current_type))\n",
    "                    current_entity = []\n",
    "                current_type = None\n",
    "\n",
    "    if current_entity:\n",
    "        entities.append((current_start_index, index, current_type))\n",
    "\n",
    "    return entities\n",
    "\n",
    "tagged_words = get_token_entity(train_esp)[3]\n",
    "\n",
    "entities = extract_entities(tagged_words, \"BIO\")\n",
    "print(entities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear un model amb la nova feature function i codificació BIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quina forma tenen les nostres dades:  [('Melbourne', 'NP', 'B-LOC'), ('(', 'Fpa', 'O'), ('Australia', 'NP', 'B-LOC'), (')', 'Fpt', 'O'), (',', 'Fc', 'O'), ('25', 'Z', 'O'), ('may', 'NC', 'O'), ('(', 'Fpa', 'O'), ('EFE', 'NC', 'B-ORG'), (')', 'Fpt', 'O'), ('.', 'Fp', 'O')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Quina forma tenen les nostres dades: \",train_esp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def get_clean_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Receives a list and returns the first position of every element, corresponing to the token\n",
    "    \"\"\"\n",
    "    clean = []\n",
    "    for element in sentence: \n",
    "        clean.append(element[0])\n",
    "\n",
    "    return clean\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Melbourne', {'bias': 1.0, 'has_capitalization': True, 'has_digit': False, 'has_punctuation': False, 'all_caps': False, 'is_capitalized': True, 'prefix': 'Mel', 'suffix': 'rne'}], 'B-LOC']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "train_esp_bio = []\n",
    "\n",
    "for sent in train_esp: \n",
    "    i = 0\n",
    "    clean_sentence = get_clean_sentence(sent)\n",
    "    whole_sentence = []\n",
    "    for token, tag, bio in sent:\n",
    "        whole_sentence.append([[token, feature_getter.get_features(clean_sentence, i)], bio])\n",
    "        i = i  + 1\n",
    "    train_esp_bio.append(whole_sentence)\n",
    "\n",
    "print(train_esp_bio[0][0])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['La', {'bias': 1.0, 'has_capitalization': True, 'has_digit': False, 'has_punctuation': False, 'all_caps': False, 'is_capitalized': True, 'prefix': 'La', 'suffix': 'La'}], 'B-LOC'], [['Coruña', {'bias': 1.0, 'has_capitalization': True, 'has_digit': False, 'has_punctuation': False, 'all_caps': False, 'is_capitalized': True, 'prefix': 'Cor', 'suffix': 'uña'}], 'I-LOC'], [[',', {'bias': 1.0, 'has_capitalization': False, 'has_digit': False, 'has_punctuation': True, 'all_caps': False, 'is_capitalized': False, 'prefix': ',', 'suffix': ','}], 'O'], [['23', {'bias': 1.0, 'has_capitalization': False, 'has_digit': True, 'has_punctuation': False, 'all_caps': False, 'is_capitalized': False, 'prefix': '23', 'suffix': '23'}], 'O'], [['may', {'bias': 1.0, 'has_capitalization': False, 'has_digit': False, 'has_punctuation': False, 'all_caps': False, 'is_capitalized': False, 'prefix': 'may', 'suffix': 'may'}], 'O'], [['(', {'bias': 1.0, 'has_capitalization': False, 'has_digit': False, 'has_punctuation': True, 'all_caps': False, 'is_capitalized': False, 'prefix': '(', 'suffix': '('}], 'O'], [['EFECOM', {'bias': 1.0, 'has_capitalization': True, 'has_digit': False, 'has_punctuation': False, 'all_caps': True, 'is_capitalized': True, 'prefix': 'EFE', 'suffix': 'COM'}], 'B-ORG'], [[')', {'bias': 1.0, 'has_capitalization': False, 'has_digit': False, 'has_punctuation': True, 'all_caps': False, 'is_capitalized': False, 'prefix': ')', 'suffix': ')'}], 'O'], [['.', {'bias': 1.0, 'has_capitalization': False, 'has_digit': False, 'has_punctuation': True, 'all_caps': False, 'is_capitalized': False, 'prefix': '.', 'suffix': '.'}], 'O']]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"test_esp_bio = []\n",
    "\n",
    "for sent in test_esp: \n",
    "    i = 0\n",
    "    clean_sentence = get_clean_sentence(sent)\n",
    "    whole_sentence = []\n",
    "    for token, tag, bio in sent:\n",
    "        whole_sentence.append([[token, feature_getter.get_features(clean_sentence, i)], bio])\n",
    "        i = i  + 1\n",
    "    test_esp_bio.append(whole_sentence)\n",
    "print(test_esp_bio[0])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ct = CRFTagger(feature_func=feature_getter.get_features)\n",
    "\n",
    "train_esp = get_token_entity(train_esp)\n",
    "\n",
    "ct.train(train_esp, 'model.crf.tagger')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_pred = ct.tag_sents(get_token(test_esp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.710512\n",
      "Recall: 0.710512\n",
      "F1 Score: 0.710512\n"
     ]
    }
   ],
   "source": [
    "\n",
    "precision, recall, f1_score = evaluate_entities(y_real, y_pred, print_errors=False)\n",
    "print(f'Precision: {precision:.6f}')\n",
    "print(f'Recall: {recall:.6f}')\n",
    "print(f'F1 Score: {f1_score:.6f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificació IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('La', 'O'), ('petición', 'O'), ('del', 'O'), ('Abogado', 'B-PER'), ('General', 'I-PER'), ('tiene', 'O'), ('lugar', 'O'), ('después', 'O'), ('de', 'O'), ('que', 'O'), ('un', 'O'), ('juez', 'O'), ('del', 'O'), ('Tribunal', 'B-ORG'), ('Supremo', 'I-ORG'), ('del', 'O'), ('estado', 'O'), ('de', 'O'), ('Victoria', 'B-LOC'), ('(', 'O'), ('Australia', 'B-LOC'), (')', 'O'), ('se', 'O'), ('viera', 'O'), ('forzado', 'O'), ('a', 'O'), ('disolver', 'O'), ('un', 'O'), ('jurado', 'O'), ('popular', 'O'), ('y', 'O'), ('suspender', 'O'), ('el', 'O'), ('proceso', 'O'), ('ante', 'O'), ('el', 'O'), ('argumento', 'O'), ('de', 'O'), ('la', 'O'), ('defensa', 'O'), ('de', 'O'), ('que', 'O'), ('las', 'O'), ('personas', 'O'), ('que', 'O'), ('lo', 'O'), ('componían', 'O'), ('podían', 'O'), ('haber', 'O'), ('obtenido', 'O'), ('información', 'O'), ('sobre', 'O'), ('el', 'O'), ('acusado', 'O'), ('a', 'O'), ('través', 'O'), ('de', 'O'), ('la', 'O'), ('página', 'O'), ('CrimeNet', 'B-MISC'), ('.', 'O')]\n",
      "[('La', 'O'), ('petición', 'O'), ('del', 'O'), ('Abogado', 'I-PER'), ('General', 'I-PER'), ('tiene', 'O'), ('lugar', 'O'), ('después', 'O'), ('de', 'O'), ('que', 'O'), ('un', 'O'), ('juez', 'O'), ('del', 'O'), ('Tribunal', 'I-ORG'), ('Supremo', 'I-ORG'), ('del', 'O'), ('estado', 'O'), ('de', 'O'), ('Victoria', 'I-LOC'), ('(', 'O'), ('Australia', 'I-LOC'), (')', 'O'), ('se', 'O'), ('viera', 'O'), ('forzado', 'O'), ('a', 'O'), ('disolver', 'O'), ('un', 'O'), ('jurado', 'O'), ('popular', 'O'), ('y', 'O'), ('suspender', 'O'), ('el', 'O'), ('proceso', 'O'), ('ante', 'O'), ('el', 'O'), ('argumento', 'O'), ('de', 'O'), ('la', 'O'), ('defensa', 'O'), ('de', 'O'), ('que', 'O'), ('las', 'O'), ('personas', 'O'), ('que', 'O'), ('lo', 'O'), ('componían', 'O'), ('podían', 'O'), ('haber', 'O'), ('obtenido', 'O'), ('información', 'O'), ('sobre', 'O'), ('el', 'O'), ('acusado', 'O'), ('a', 'O'), ('través', 'O'), ('de', 'O'), ('la', 'O'), ('página', 'O'), ('CrimeNet', 'I-MISC'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "def bio_to_io(bio_tagged_sentences):\n",
    "    \"\"\"\n",
    "    Converteix les etiquetes de múltiples frases des de la codificació BIO a IO.\n",
    "    \n",
    "    Arguments:\n",
    "        bio_tagged_sentences: Una llista de llistes de tuples (word, tag) on 'tag' és en codificació BIO.\n",
    "    \n",
    "    Retorna:\n",
    "        Una llista de llistes de tuples (word, io_tag) on 'io_tag' és en codificació IO per cada frase.\n",
    "    \"\"\"\n",
    "    io_tagged_sentences = []\n",
    "    \n",
    "    for sentence in bio_tagged_sentences:\n",
    "        io_tagged_sentence = []\n",
    "        for word, tag in sentence:\n",
    "            if tag.startswith('B-'):\n",
    "                # Canvia B- per I-\n",
    "                io_tagged_sentence.append((word, 'I-' + tag[2:]))\n",
    "            elif tag.startswith('I-'):\n",
    "                io_tagged_sentence.append((word, tag))\n",
    "            else:\n",
    "                # Manté les etiquetes 'O' tal com estan\n",
    "                io_tagged_sentence.append((word, 'O'))\n",
    "        io_tagged_sentences.append(io_tagged_sentence)\n",
    "    \n",
    "    return io_tagged_sentences\n",
    "\n",
    "\n",
    "tagged_words = get_token_entity(train_esp)\n",
    "tagged_words_io = bio_to_io(tagged_words)\n",
    "print(tagged_words[3])\n",
    "print(tagged_words_io[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from nltk.tag import CRFTagger\n",
    "import pycrfsuite\n",
    "\n",
    "ct = CRFTagger(feature_func=None)\n",
    "#train and test sets without the postag\n",
    "train_esp_bio = get_token_entity(train_esp)\n",
    "test_esp_bio = get_token_entity(test_esp)\n",
    "print(train_esp_bio[0])\n",
    "\n",
    "#y_test is the true labels\n",
    "y_test = [[iob for word, iob in sent] for sent in test_esp_bio]\n",
    "print(y_test[0])\n",
    "\n",
    "#train the model\n",
    "ct.train(train_esp_bio, 'model.crf.tagger')\n",
    "\n",
    "#predict the labels\n",
    "y_pred = ct.tag_sents([[word for word, iob in sent] for sent in test_esp_bio])\n",
    "y_pred = [[iob for word, iob in sent] for sent in y_pred]\n",
    "print(y_pred[0])\n",
    "\n",
    "\n",
    "#show the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Flatten y_test and y_pred\n",
    "y_test_flat = [iob for sent in y_test for iob in sent]\n",
    "y_pred_flat = [iob for sent in y_pred for iob in sent]\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test_flat, y_pred_flat)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()\n",
    "\n",
    "#show the classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test_flat, y_pred_flat))\n",
    "ct.accuracy(test_esp_bio)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def evaluation(true_entities, pred_entities):\n",
    "    \"\"\"\n",
    "    Avalua la predicció de les entitats amb precisió, record i F1-score.\n",
    "    \"\"\"\n",
    "    true_positives = len(set(true_entities) & set(pred_entities))\n",
    "    if true_positives == 0:\n",
    "        return 0, 0, 0\n",
    "    precision = true_positives / len(pred_entities)\n",
    "    recall = true_positives / len(true_entities)\n",
    "    f1_score = 2 * precision * recall / (precision + recall)\n",
    "    return precision, recall, f1_score\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import CRFTagger\n",
    "import pycrfsuite\n",
    "\n",
    "ct = CRFTagger(feature_func=feature_getter.get_features)\n",
    "\n",
    "train_esp_bio = get_token_entity(train_esp)\n",
    "test_esp_bio = get_token_entity(test_esp)\n",
    "\n",
    "ct.train(train_esp_bio, 'model.crf.tagger')\n",
    "\n",
    "# Test the model\n",
    "test_pred = ct.tag_sents(get_token(test_esp))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
