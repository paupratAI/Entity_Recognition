{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pràctica 3 - PLH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realitzada pels alumnes Lluc Furriols i Pau Prat Moreno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os \\nf = open(\"/dev/null\", \"w\")\\nos.dup2(f.fileno(), 2)\\nf.close()\\n\\nimport nltk\\nimport ssl\\n\\ntry:\\n    _create_unverified_https_context = ssl._create_unverified_context\\nexcept AttributeError:\\n    pass\\nelse:\\n    ssl._create_default_https_context = _create_unverified_https_context\\n\\nnltk.download()\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import os \n",
    "f = open(\"/dev/null\", \"w\")\n",
    "os.dup2(f.fileno(), 2)\n",
    "f.close()\n",
    "\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importació de llibreries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt', quiet=True) # Tokenitzador\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True) # Etiquetador POS\n",
    "nltk.download('maxent_ne_chunker', quiet=True) # Etiquetador Entitats Anomenades\n",
    "nltk.download('words', quiet=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carreguem les dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2002 to /Users/pau/nltk_data...\n",
      "[nltk_data]   Package conll2002 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('conll2002')\n",
    "from nltk.corpus import conll2002\n",
    "\n",
    "train_esp = conll2002.iob_sents('esp.train') # Train, \n",
    "val_esp = conll2002.iob_sents('esp.testa') # Val\n",
    "test_esp = conll2002.iob_sents('esp.testb') # Test\n",
    "\n",
    "train_ned = conll2002.iob_sents('ned.train') # Train\n",
    "val_ned = conll2002.iob_sents('ned.testa') # Val\n",
    "test_ned = conll2002.iob_sents('ned.testb') # Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcions essencials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token(sequence):\n",
    "    \"\"\"\n",
    "    Retorna una llista de tokens.\n",
    "    \"\"\"\n",
    "    return [[(token) for token, pos, entity in sentence] for sentence in sequence]\n",
    "\n",
    "def get_token_POS(sequence):\n",
    "    \"\"\"\n",
    "    Retorna una llista de tokens i el seu POS tag.\n",
    "    \"\"\"\n",
    "    return [[(token, pos) for token, pos, entity in sentence] for sentence in sequence]\n",
    "\n",
    "def get_token_entity(sequence):\n",
    "    \"\"\"\n",
    "    Retorna una llista de tokens i les seves entitats.\n",
    "    \"\"\"\n",
    "    return [[(token, entity) for token, pos, entity in sentence] for sentence in sequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(tagged_words, encoding='BIO'):\n",
    "    \"\"\"\n",
    "    Extreu les entitats d'una llista de paraules etiquetades segons l'encoding especificat.\n",
    "    \n",
    "    Arguments:\n",
    "        tagged_words: Una llista de tuples (word, tag), on 'word' és una paraula del text i 'tag' és la seva etiqueta (BIO/BIOW/IO).\n",
    "        encoding: El tipus de codificació utilitzat per les etiquetes ('BIO', 'BIOW', 'IO').\n",
    "        \n",
    "    Retorna:\n",
    "        Una llista de tuples (index_inicial, index_final, etiqueta) que representen les entitats trobades.\n",
    "    \"\"\"\n",
    "    entities = []  # Llista on es guardaran les entitats trobades\n",
    "    current_entity = []  # Guarda les paraules de l'entitat actual\n",
    "    current_type = None  # Tipus de l'entitat actual\n",
    "    current_start_index = None  # Índex d'inici de l'entitat actual\n",
    "\n",
    "    # Recorre cada paraula i la seva etiqueta en la llista de paraules etiquetades\n",
    "    for index, (word, tag) in enumerate(tagged_words):\n",
    "        # Determina el tipus de tag, o 'None' si el tag és 'O'\n",
    "        tag_type = None if tag == 'O' else tag[2:]\n",
    "        # Obté el prefix del tag, o 'None' si és 'O'\n",
    "        tag_prefix = tag[0] if tag != 'O' else None\n",
    "\n",
    "        # Finalitza l'entitat actual si es troba un tag 'O' o si canvia el tipus d'entitat\n",
    "        if tag == 'O' or tag_prefix not in ['B', 'I', 'W'] or (tag_prefix == 'I' and tag_type != current_type):\n",
    "            if current_entity:\n",
    "                entities.append((current_start_index, index - 1, current_type))\n",
    "                current_entity = []\n",
    "                current_type = None\n",
    "        if tag == 'O':\n",
    "            continue\n",
    "\n",
    "        # Gestiona els canvis d'entitat segons l'encoding\n",
    "        if encoding == 'IO' and tag_type != current_type:\n",
    "            if current_entity:\n",
    "                entities.append((current_start_index, index - 1, current_type))\n",
    "            current_entity = [word]\n",
    "            current_start_index = index\n",
    "            current_type = tag_type\n",
    "        elif tag_prefix in ['B', 'W']:\n",
    "            if current_entity:\n",
    "                entities.append((current_start_index, index - 1, current_type))\n",
    "            current_entity = [word]\n",
    "            current_start_index = index\n",
    "            current_type = tag_type\n",
    "            if tag_prefix == 'W':\n",
    "                # Si el tag és 'W-', afegeix immediatament com a entitat de paraula única\n",
    "                entities.append((current_start_index, index, current_type))\n",
    "                current_entity = []\n",
    "                current_type = None\n",
    "        elif tag_prefix == 'I' and current_type == tag_type:\n",
    "            current_entity.append(word)\n",
    "\n",
    "    # Afegeix la darrera entitat si encara hi ha una pendent\n",
    "    if current_entity:\n",
    "        entities.append((current_start_index, index, current_type))\n",
    "\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_entities(true_entities, pred_entities):\n",
    "    \"\"\"\n",
    "    Avaluació de les entitats reconegudes comparant conjunts d'entitats.\n",
    "\n",
    "    Args:\n",
    "    true_entities (list): Llista de tuples representant les entitats reals (start, end, type).\n",
    "    pred_entities (list): Llista de tuples representant les entitats predites (start, end, type).\n",
    "\n",
    "    Returns:\n",
    "    dict: Un diccionari amb les mètriques 'precision', 'recall', i 'f1_score'.\n",
    "    \"\"\"\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    for true_sent, pred_sent in zip(true_entities, pred_entities):\n",
    "        true_set = set(true_sent)\n",
    "        pred_set = set(pred_sent)\n",
    "\n",
    "        true_positives += len(true_set & pred_set)\n",
    "        false_positives += len(pred_set - true_set)\n",
    "        false_negatives += len(true_set - pred_set)\n",
    "\n",
    "    precisio = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    f1_score = 2 * precisio * recall / (precisio + recall) if (precisio + recall) > 0 else 0\n",
    "\n",
    "    # Arrodonim els valors a 3 decimals\n",
    "    precisio, recall, f1_score= round(precisio, 3), round(recall, 3), round(f1_score, 3)\n",
    "\n",
    "    return {\n",
    "        'precisio': precisio,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models predictor de  postags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crf import *\n",
    "import re\n",
    "import string\n",
    "\n",
    "class FeatureGetterPOS:\n",
    "    def __init__(self):\n",
    "        # Compila un patró regex per detectar dígits\n",
    "        self._pattern = re.compile(r\"\\d\")\n",
    "\n",
    "    def has_digit(self, token):\n",
    "        \"\"\"\n",
    "        Comprova si el token conté algun dígit.\n",
    "        \"\"\"\n",
    "        return any(char.isdigit() for char in token)\n",
    "\n",
    "    def has_punctuation(self, token):\n",
    "        \"\"\"\n",
    "        Comprova si el token conté algun signe de puntuació.\n",
    "        \"\"\"\n",
    "        return any(char in string.punctuation for char in token)\n",
    "\n",
    "    def get_prefix(self, token, n=3):\n",
    "        \"\"\"\n",
    "        Obté el prefix del token.\n",
    "        \"\"\"\n",
    "        return token[:n] if len(token) > n else token\n",
    "\n",
    "    def get_suffix(self, token, n=3):\n",
    "        \"\"\"\n",
    "        Obté el sufix del token.\n",
    "        \"\"\"\n",
    "        return token[-n:] if len(token) > n else token\n",
    "    \n",
    "\n",
    "    def get_features(self, tokens, index):\n",
    "        \"\"\"\n",
    "        Obté les característiques del token.\n",
    "        \"\"\"\n",
    "        token = tokens[index]\n",
    "        features = [\"WORD_\" + token]\n",
    "\n",
    "        if token[0].isupper():\n",
    "            features.append(\"CAPITALIZATION\")\n",
    "\n",
    "        if self.has_digit(token):\n",
    "            features.append(\"HAS_NUM\")\n",
    "\n",
    "        if self.has_punctuation(token):\n",
    "            features.append(\"PUNCTUATION\")\n",
    "            \n",
    "        features.extend([\"SUF_\" + self.get_suffix(token, n) for n in range(1, 4) if len(token) >= n])\n",
    "        features.extend([\"PRE_\" + self.get_prefix(token, n) for n in range(1, 4) if len(token) >= n])\n",
    "        if index > 0:\n",
    "            prev_token = tokens[index - 1]\n",
    "            features.append(\"PREV_WORD_\" + prev_token)\n",
    "        if index < len(tokens) - 1:\n",
    "            next_token = tokens[index + 1]\n",
    "            features.append(\"NEXT_WORD_\" + next_token)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT: La següent cel·la de codi és computacionalment costosa d'executar, és per això que a la cel·la que es troba a continuació carreguem els models ja entrenats prèviament, i així estalviar-nos el temps d'execució que comporta tornar a entrenar els models ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom nltk.tag import CRFTagger\\n\\n# Diccionari amb les dades d'entrenament per cada idioma\\ntrain_data = {\\n    'esp': train_esp,\\n    'ned': train_ned\\n}\\n\\nfor lang in ['esp', 'ned']:\\n    # Obtenim les dades d'entrenament amb les etiquetes POS\\n    train_lang_pos = get_token_POS(train_data[lang])\\n\\n    # Creem i configurem el model\\n    feature_getter_pos = FeatureGetterPOS() \\n    ct_POS = CRFTagger(feature_func=feature_getter_pos.get_features)\\n\\n    # Entrenem el model\\n    model_filename = f'model.crf.taggerPOS_{lang}'\\n    ct_POS.train(train_lang_pos, model_filename)  \\n\\n    print(f'Model per {lang} entrenat i guardat com {model_filename}')\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from nltk.tag import CRFTagger\n",
    "\n",
    "# Diccionari amb les dades d'entrenament per cada idioma\n",
    "train_data = {\n",
    "    'esp': train_esp,\n",
    "    'ned': train_ned\n",
    "}\n",
    "\n",
    "for lang in ['esp', 'ned']:\n",
    "    # Obtenim les dades d'entrenament amb les etiquetes POS\n",
    "    train_lang_pos = get_token_POS(train_data[lang])\n",
    "\n",
    "    # Creem i configurem el model\n",
    "    feature_getter_pos = FeatureGetterPOS() \n",
    "    ct_POS = CRFTagger(feature_func=feature_getter_pos.get_features)\n",
    "\n",
    "    # Entrenem el model\n",
    "    model_filename = f'model.crf.taggerPOS_{lang}'\n",
    "    ct_POS.train(train_lang_pos, model_filename)  \n",
    "\n",
    "    print(f'Model per {lang} entrenat i guardat com {model_filename}')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import CRFTagger\n",
    "\n",
    "# Carreguem i configurem un CRFTagger per cada idioma\n",
    "feature_getter_pos = FeatureGetterPOS()\n",
    "\n",
    "ct_POS_esp = CRFTagger(feature_func=feature_getter_pos.get_features)\n",
    "ct_POS_esp.set_model_file('model.crf.taggerPOS_esp')\n",
    "\n",
    "ct_POS_ned = CRFTagger(feature_func=feature_getter_pos.get_features)\n",
    "ct_POS_ned.set_model_file('model.crf.taggerPOS_ned')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara ja tenim els models predictors dels POS tags per cadascun dels dos idiomes:\n",
    "* ct_POS_esp\n",
    "* ct_POS_ned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esp: 0.961\n",
      "Ned: 0.958\n"
     ]
    }
   ],
   "source": [
    "# Funció per avaluar el POS tagging\n",
    "def evaluate_POS(true_tags, pred_tags):\n",
    "    \"\"\"\n",
    "    Avaluació del POS tagging comparant seqüències de tags.\n",
    "\n",
    "    Args:\n",
    "    true_tags (list): Llista de llistes amb els tags reals.\n",
    "    pred_tags (list): Llista de llistes amb els tags predits.\n",
    "\n",
    "    Returns:\n",
    "    dict: Un diccionari amb les mètriques 'accuracy'.\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for true_sent, pred_sent in zip(true_tags, pred_tags):\n",
    "        total += len(true_sent)\n",
    "        correct += sum(1 for true_tag, pred_tag in zip(true_sent, pred_sent) if true_tag == pred_tag)\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "\n",
    "    return round(accuracy, 3)\n",
    "  \n",
    "\n",
    "# Prediccions del POS tagging - Espanyol\n",
    "y_pred_POS_esp = ct_POS_esp.tag_sents(get_token(test_esp))\n",
    "y_real_POS_esp = get_token_POS(test_esp)\n",
    "print(f\"Esp: {evaluate_POS(y_real_POS_esp, y_pred_POS_esp)}\")\n",
    "\n",
    "# Prediccions del POS tagging - Neerlandés\n",
    "y_pred_POS_ned = ct_POS_ned.tag_sents(get_token(test_ned))\n",
    "y_real_POS_ned = get_token_POS(test_ned)\n",
    "print(f\"Ned: {evaluate_POS(y_real_POS_ned, y_pred_POS_ned)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El model prediu el postags amb una accuracy al voltant d'un 96%. Com que és un bon resultat, l'utilitzarem en les feature functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model predicor d'entitats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquesta classe és igual que la anterior, però utilitza el model anterior per predir els postags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "class FeatureGetter:\n",
    "    def __init__(self, ct_POS, use_digit=True, use_punctuation=True, use_prefix=True, use_suffix=True, use_capitalization=True, use_prev_word=True, use_next_word=True):\n",
    "        \"\"\"\n",
    "        Inicialitza el FeatureGetter amb opcions per activar o desactivar certes característiques.\n",
    "        \"\"\"\n",
    "        self.ct_POS = ct_POS\n",
    "        self.use_digit = use_digit\n",
    "        self.use_punctuation = use_punctuation\n",
    "        self.use_prefix = use_prefix\n",
    "        self.use_suffix = use_suffix\n",
    "        self.use_capitalization = use_capitalization\n",
    "        self.use_prev_word = use_prev_word\n",
    "        self.use_next_word = use_next_word\n",
    "        self._pattern = re.compile(r\"\\d\")\n",
    "\n",
    "    def has_digit(self, token):\n",
    "        \"\"\"\n",
    "        Comprova si el token conté algun dígit.\n",
    "        \"\"\"\n",
    "        return any(char.isdigit() for char in token)\n",
    "\n",
    "    def has_punctuation(self, token):\n",
    "        \"\"\"\n",
    "        Comprova si el token conté algun signe de puntuació.\n",
    "        \"\"\"\n",
    "        return any(char in string.punctuation for char in token)\n",
    "\n",
    "    def get_prefix(self, token, n=3):\n",
    "        \"\"\"\n",
    "        Obté el prefix del token.\n",
    "        \"\"\"\n",
    "        return token[:n] if len(token) > n else token\n",
    "\n",
    "    def get_suffix(self, token, n=3):\n",
    "        \"\"\"\n",
    "        Obté el sufix del token.\n",
    "        \"\"\"\n",
    "        return token[-n:] if len(token) > n else token\n",
    "    \n",
    "    def pos_tag(self, token):\n",
    "        \"\"\"\n",
    "        Etiqueta POS del token.\n",
    "        \"\"\"\n",
    "        return self.ct_POS.tag([token])[0][1]\n",
    "\n",
    "\n",
    "    def get_features(self, tokens, index):\n",
    "        \"\"\"\n",
    "        Obté les característiques del token.\n",
    "        \"\"\"\n",
    "        token = tokens[index]\n",
    "        features = [\"WORD_\" + token]\n",
    "\n",
    "        if self.use_capitalization and token[0].isupper():\n",
    "            features.append(\"CAPITALIZATION\")\n",
    "\n",
    "        if self.use_digit and self.has_digit(token):\n",
    "            features.append(\"HAS_NUM\")\n",
    "\n",
    "        if self.use_punctuation and self.has_punctuation(token):\n",
    "            features.append(\"PUNCTUATION\")\n",
    "\n",
    "        if self.use_suffix:\n",
    "            features.extend([\"SUF_\" + self.get_suffix(token, n) for n in range(1, 4) if len(token) >= n])\n",
    "\n",
    "        if self.use_prefix:\n",
    "            features.extend([\"PRE_\" + self.get_prefix(token, n) for n in range(1, 4) if len(token) >= n])\n",
    "\n",
    "        pos_tag = self.pos_tag(token)\n",
    "        features.append(\"POS_\" + pos_tag)\n",
    "        if self.use_prev_word and index > 0:\n",
    "            prev_token = tokens[index - 1]\n",
    "            features.append(\"PREV_WORD_\" + prev_token)\n",
    "\n",
    "        if self.use_next_word and index < len(tokens) - 1:\n",
    "            next_token = tokens[index + 1]\n",
    "            features.append(\"NEXT_WORD_\" + next_token)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search - Optimitar Feature Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realitzem un Grid Search per trobar la millor combinació de feature functions tal que es maximitza el recall.\n",
    "\n",
    "Com que les dades inicials estan en codificació BIO, aquesta cerca en quadrícula es realitza amb les dades en aquesta mateixa codificació."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from nltk.tag import CRFTagger\n",
    "\n",
    "def grid_search_feature_combinations(train_data, val_data, feature_combinations):\n",
    "    \"\"\"\n",
    "    Realitza un grid search amb un conjunt definit de combinacions de funcions de característiques per trobar la millor que maximitza el recall.\n",
    "    \n",
    "    Args:\n",
    "    train_data (list): Dades d'entrenament en format BIO.\n",
    "    val_data (list): Dades de validació en format BIO.\n",
    "    feature_combinations (list): Una llista de diccionaris amb combinacions de funcions de característiques.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: La millor combinació de característiques i el seu valor de recall.\n",
    "    \"\"\"\n",
    "    best_recall = 0\n",
    "    best_combination = None\n",
    "    \n",
    "    train_data = get_token_entity(train_data)\n",
    "    \n",
    "    for options in feature_combinations:\n",
    "        feature_getter = FeatureGetter(**options, ct_POS=ct_POS)\n",
    "        ct = CRFTagger(feature_func=feature_getter.get_features)\n",
    "        \n",
    "        # Entrena el model CRFTagger amb les dades d'entrenament\n",
    "        ct.train(train_data, 'model.crf.tagger')\n",
    "        \n",
    "        # Prepara les dades de test i obtenir les prediccions\n",
    "        y_real = [extract_entities(sent) for sent in get_token_entity(val_data)]\n",
    "        val_tokens = get_token(val_data)\n",
    "        y_pred = ct.tag_sents(val_tokens)\n",
    "        y_pred_entities = [extract_entities(sent) for sent in y_pred]\n",
    "        \n",
    "        # Avaluació de les entitats predites respecte a les reals\n",
    "        metrics = evaluate_entities(y_real, y_pred_entities)\n",
    "        recall = metrics['recall']\n",
    "        \n",
    "        # Actualitza la millor combinació si el recall actual és millor\n",
    "        if recall > best_recall:\n",
    "            best_recall = recall\n",
    "            best_combination = options\n",
    "\n",
    "    return best_combination, best_recall\n",
    "\n",
    "# Defineix les combinacions de paràmetres explícitament\n",
    "feature_combinations = [\n",
    "    {'use_prefix': True, 'use_suffix': True, 'use_prev_word': True, 'use_next_word': True},\n",
    "    {'use_prefix': True, 'use_suffix': True, 'use_prev_word': False, 'use_next_word': False},\n",
    "    {'use_prefix': False, 'use_suffix': False, 'use_prev_word': True, 'use_next_word': True}\n",
    "]\n",
    "\n",
    "#best_combination, best_recall = grid_search_feature_combinations(train_esp, val_esp, feature_combinations)\n",
    "#print(\"Best feature combination:\", best_combination)\n",
    "#print(\"Best recall:\", best_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cel·la anterior té un temps d'execució molt elevat, per la qual cosa si es desitja executar, només cal descomentar les últimes tres línies.\n",
    "Per tal d'estalviar tot aquest temps de còmput, els resultats obtinguts són els següents:\n",
    "\n",
    "* Best feature combination: {'use_prefix': True, 'use_suffix': True, 'use_prev_word': True, 'use_next_word': True}\n",
    "* Best recall: 0.711"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com podem veure, la combinació que dona un recall més elevat és amb totes les feature functions actives, per la qual cosa les utilitzarem en el model final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codificació BIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import CRFTagger\n",
    "import pycrfsuite\n",
    "import pandas as pd\n",
    "\n",
    "def compare_models(train_data, test_data, feature_func=None, lang='esp'):\n",
    "    \"\"\"\n",
    "    Entrena i evalua dos models CRFTagger: un bàsic i un altre amb la funció _get_features()\n",
    "    modificada, mostrant les mètriques de precisio, recall i F1 en una taula.\n",
    "\n",
    "    Args:\n",
    "    train_data (list): Dades d'entrenament en format BIO.\n",
    "    test_data (list): Dades de prova en format BIO.\n",
    "    feature_func (callable, optional): Funció de característiques per al segon model.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Taula amb les mètriques de precisio, recall i F1 per a cada model.\n",
    "    \"\"\"\n",
    "    # Entrenament del model bàsic\n",
    "    ct_basic = CRFTagger(feature_func=None)\n",
    "    ct_basic.train(train_data, f'model.crf.tagger_BIO_{lang}')\n",
    "    \n",
    "    # Entrenament del model modificat si feature_func és proporcionada\n",
    "    ct_modified = CRFTagger(feature_func=feature_func)\n",
    "    ct_modified.train(train_data, f'model.crf.tagger_BIO_{lang}')\n",
    "    \n",
    "    # Preparació de les dades de prova i predicció per a cada model\n",
    "    y_real = [extract_entities(sent) for sent in get_token_entity(test_data)]\n",
    "    y_pred_basic = ct_basic.tag_sents(get_token(test_data))\n",
    "    y_pred_modified = ct_modified.tag_sents(get_token(test_data))\n",
    "    \n",
    "    y_pred_basic_entities = [extract_entities(sent) for sent in y_pred_basic]\n",
    "    y_pred_modified_entities = [extract_entities(sent) for sent in y_pred_modified]\n",
    "    \n",
    "    # Avaluació de les entitats per a cada model\n",
    "    metrics_basic = evaluate_entities(y_real, y_pred_basic_entities)\n",
    "    metrics_modified = evaluate_entities(y_real, y_pred_modified_entities)\n",
    "    \n",
    "    # Creació de la taula de resultats\n",
    "    results = {\n",
    "        'Metric': ['Precisio', 'Recall', 'F1 Score'],\n",
    "        'CRFTagger Basic': [metrics_basic['precisio'], metrics_basic['recall'], metrics_basic['f1_score']],\n",
    "        'CRFTagger Modified': [metrics_modified['precisio'], metrics_modified['recall'], metrics_modified['f1_score']]\n",
    "    }\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultats per Espanyol:\n",
      "     Metric  CRFTagger Basic  CRFTagger Modified\n",
      "0  Precisio            0.741               0.784\n",
      "1    Recall            0.707               0.769\n",
      "2  F1 Score            0.723               0.776\n",
      "Resultats per Neerlandès:\n",
      "     Metric  CRFTagger Basic  CRFTagger Modified\n",
      "0  Precisio            0.701               0.769\n",
      "1    Recall            0.621               0.713\n",
      "2  F1 Score            0.659               0.740\n"
     ]
    }
   ],
   "source": [
    "# Ús de la funció per a Espanyol i Neerlandès\n",
    "train_esp_data = get_token_entity(train_esp)\n",
    "feature_getter_esp = FeatureGetter(ct_POS=ct_POS_esp)\n",
    "\n",
    "train_ned_data = get_token_entity(train_ned)\n",
    "feature_getter_ned = FeatureGetter(ct_POS=ct_POS_ned)\n",
    "\n",
    "# Resultats per Espanyol\n",
    "results_esp = compare_models(train_esp_data, test_esp, feature_getter_esp.get_features, 'esp')\n",
    "print(\"Resultats per Espanyol:\")\n",
    "print(results_esp)\n",
    "\n",
    "# Resultats per Neerlandès\n",
    "results_ned = compare_models(train_ned_data, test_ned, feature_getter_ned.get_features, 'ned')\n",
    "print(\"Resultats per Neerlandès:\")\n",
    "print(results_ned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codificació IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bio_to_io(bio_tagged_sentences):\n",
    "    \"\"\"\n",
    "    Converteix les etiquetes de múltiples frases des de la codificació BIO a IO.\n",
    "    \n",
    "    Arguments:\n",
    "        bio_tagged_sentences: Una llista de llistes de tuples (word, tag) on 'tag' és en codificació BIO.\n",
    "    \n",
    "    Retorna:\n",
    "        Una llista de llistes de tuples (word, io_tag) on 'io_tag' és en codificació IO per cada frase.\n",
    "    \"\"\"\n",
    "    io_tagged_sentences = []\n",
    "    \n",
    "    for sentence in bio_tagged_sentences:\n",
    "        io_tagged_sentence = []\n",
    "        for word, tag in sentence:\n",
    "            if tag.startswith('B-'):\n",
    "                # Canvia B- per I-\n",
    "                io_tagged_sentence.append((word, 'I-' + tag[2:]))\n",
    "            elif tag.startswith('I-'):\n",
    "                io_tagged_sentence.append((word, tag))\n",
    "            else:\n",
    "                # Manté les etiquetes 'O' tal com estan\n",
    "                io_tagged_sentence.append((word, 'O'))\n",
    "        io_tagged_sentences.append(io_tagged_sentence)\n",
    "    \n",
    "    return io_tagged_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenar el model amb la codificació IO i avaluar els resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_encoding(encoding, train_data, test_data, feature_getter, lang):\n",
    "    \"\"\"\n",
    "    Processa les dades, entrena un model, fa prediccions, i evalua el rendiment.\n",
    "    \n",
    "    Args:\n",
    "    lang (str): Tipus de codificació de les dades.\n",
    "    train_data (list): Dades d'entrenament en format BIO.\n",
    "    test_data (list): Dades de prova en format BIO.\n",
    "    feature_getter (FeatureGetter): Instància de la classe FeatureGetter.\n",
    "    lang (str): Idioma de les dades.\n",
    "\n",
    "    Returns:\n",
    "    None: Imprimeix les mètriques d'avaluació.\n",
    "    \"\"\"\n",
    "    # Convertim les dades d'entrenament de format BIO a l'encoding\n",
    "    if encoding == 'IO':\n",
    "        train_data = bio_to_io(get_token_entity(train_data))\n",
    "    else: # 'BIOW'\n",
    "        train_data = bio_to_biow(get_token_entity(train_data))\n",
    "\n",
    "    # Creem el model i l'entrenem amb les dades d'entrenament\n",
    "    ct = CRFTagger(feature_func=feature_getter.get_features)\n",
    "    ct.train(train_data, f'model.crf.tagger_{encoding}_{lang}')\n",
    "\n",
    "    # Convertim les dades de prova de format BIO a l'encoding i fem les prediccions\n",
    "    if encoding == 'IO':\n",
    "        y_real = bio_to_io(get_token_entity(test_data))\n",
    "        y_pred = bio_to_io(ct.tag_sents(get_token(test_data)))\n",
    "    else: \n",
    "        y_real = bio_to_biow(get_token_entity(test_data))\n",
    "        y_pred = bio_to_biow(ct.tag_sents(get_token(test_data)))\n",
    "\n",
    "    # Extraiem les entitats reals i predites\n",
    "    y_real_entities = [extract_entities(sent, f\"{encoding}\") for sent in y_real]\n",
    "    y_pred_entities = [extract_entities(sent, f\"{encoding}\") for sent in y_pred]\n",
    "\n",
    "    # Avaluem les entitats\n",
    "    metrics = evaluate_entities(y_real_entities, y_pred_entities)\n",
    "    print(f'Precisio: {metrics[\"precisio\"]:.3f}, Recall: {metrics[\"recall\"]:.3f}, F1 Score: {metrics[\"f1_score\"]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultats per Espanyol:\n",
      "Precisio: 0.785, Recall: 0.763, F1 Score: 0.774\n",
      "Resultats per Neerlandès:\n",
      "Precisio: 0.746, Recall: 0.685, F1 Score: 0.714\n"
     ]
    }
   ],
   "source": [
    "# Dades per Espanyol\n",
    "train_esp_data = train_esp\n",
    "test_esp_data = test_esp\n",
    "feature_getter_esp = FeatureGetter(ct_POS=ct_POS_esp)  \n",
    "\n",
    "# Dades per Neerlandès\n",
    "train_ned_data = train_ned\n",
    "test_ned_data = test_ned\n",
    "feature_getter_ned = FeatureGetter(ct_POS=ct_POS_ned)  \n",
    "\n",
    "# Processar i avaluar per Espanyol\n",
    "print(\"Resultats per Espanyol:\")\n",
    "evaluate_encoding(\"IO\", train_esp_data, test_esp_data, feature_getter_esp, 'esp')\n",
    "\n",
    "# Processar i avaluar per Neerlandès\n",
    "print(\"Resultats per Neerlandès:\")\n",
    "evaluate_encoding(\"IO\", train_ned_data, test_ned_data, feature_getter_ned, 'ned')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codificació BIOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bio_to_biow(bio_tagged_sentences):\n",
    "    \"\"\"\n",
    "    Converteix les etiquetes de múltiples frases des de la codificació BIO a BIOW.\n",
    "    \n",
    "    Arguments:\n",
    "        bio_tagged_sentences: Una llista de llistes de tuples (word, tag) on 'tag' és en codificació BIO.\n",
    "    \n",
    "    Retorna:\n",
    "        Una llista de llistes de tuples (word, biow_tag) on 'biow_tag' és en codificació BIOW per cada frase.\n",
    "    \"\"\"\n",
    "    biow_tagged_sentences = []\n",
    "    \n",
    "    for sentence in bio_tagged_sentences:\n",
    "        biow_tagged_sentence = []\n",
    "        for i, (word, tag) in enumerate(sentence):\n",
    "            if tag.startswith('B-'):\n",
    "                # Comprovar si l'entitat només té una paraula (W)\n",
    "                if i + 1 < len(sentence) and sentence[i + 1][1].startswith('I-'):\n",
    "                    biow_tagged_sentence.append((word, 'B-' + tag[2:]))\n",
    "                else:\n",
    "                    biow_tagged_sentence.append((word, 'W-' + tag[2:]))\n",
    "            elif tag.startswith('I-'):\n",
    "                biow_tagged_sentence.append((word, tag))\n",
    "            else:\n",
    "                # Manté les etiquetes 'O' tal com estan\n",
    "                biow_tagged_sentence.append((word, 'O'))\n",
    "        biow_tagged_sentences.append(biow_tagged_sentence)\n",
    "    \n",
    "    return biow_tagged_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenar el model amb la codificació BIOW i avaluar els resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultats per Espanyol:\n",
      "Precisio: 0.792, Recall: 0.282, F1 Score: 0.416\n",
      "Resultats per Neerlandès:\n",
      "Precisio: 0.804, Recall: 0.275, F1 Score: 0.410\n"
     ]
    }
   ],
   "source": [
    "feature_getter_esp = FeatureGetter(ct_POS=ct_POS_esp)  \n",
    "feature_getter_ned = FeatureGetter(ct_POS=ct_POS_ned)  \n",
    "\n",
    "# Processar i avaluar per Espanyol\n",
    "print(\"Resultats per Espanyol:\")\n",
    "evaluate_encoding(\"BIOW\", train_esp_data, test_esp_data, feature_getter_esp, 'esp')\n",
    "\n",
    "# Processar i avaluar per Neerlandès\n",
    "print(\"Resultats per Neerlandès:\")\n",
    "evaluate_encoding(\"BIOW\", train_ned_data, test_ned_data, feature_getter_ned, 'ned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prova amb textos reals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
