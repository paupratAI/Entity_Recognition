{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pràctica 3 - PLH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realitzada pels alumnes Lluc Furriols i Pau Prat Moreno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os \\nf = open(\"/dev/null\", \"w\")\\nos.dup2(f.fileno(), 2)\\nf.close()\\n\\nimport nltk\\nimport ssl\\n\\ntry:\\n    _create_unverified_https_context = ssl._create_unverified_context\\nexcept AttributeError:\\n    pass\\nelse:\\n    ssl._create_default_https_context = _create_unverified_https_context\\n\\nnltk.download()\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import os \n",
    "f = open(\"/dev/null\", \"w\")\n",
    "os.dup2(f.fileno(), 2)\n",
    "f.close()\n",
    "\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importació de llibreries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1002)>\n",
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify\n",
      "[nltk_data]     failed: unable to get local issuer certificate\n",
      "[nltk_data]     (_ssl.c:1002)>\n",
      "[nltk_data] Error loading maxent_ne_chunker: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1002)>\n",
      "[nltk_data] Error loading words: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1002)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt', quiet=True) # Tokenitzador\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True) # Etiquetador POS\n",
    "nltk.download('maxent_ne_chunker', quiet=True) # Etiquetador Entitats Anomenades\n",
    "nltk.download('words', quiet=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carreguem les dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading conll2002: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1002)>\n"
     ]
    }
   ],
   "source": [
    "nltk.download('conll2002')\n",
    "from nltk.corpus import conll2002\n",
    "\n",
    "train_esp = conll2002.iob_sents('esp.train') # Train, \n",
    "val_esp = conll2002.iob_sents('esp.testa') # Val\n",
    "test_esp = conll2002.iob_sents('esp.testb') # Test\n",
    "\n",
    "train_ned = conll2002.iob_sents('ned.train') # Train\n",
    "val_ned = conll2002.iob_sents('ned.testa') # Val\n",
    "test_ned = conll2002.iob_sents('ned.testb') # Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcions essencials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token(sequence):\n",
    "    \"\"\"\n",
    "    Retorna una llista de tokens.\n",
    "    \"\"\"\n",
    "    return [[(token) for token, pos, entity in sentence] for sentence in sequence]\n",
    "\n",
    "def get_token_POS(sequence):\n",
    "    \"\"\"\n",
    "    Retorna una llista de tokens i el seu POS tag.\n",
    "    \"\"\"\n",
    "    return [[(token, pos) for token, pos, entity in sentence] for sentence in sequence]\n",
    "\n",
    "def get_token_entity(sequence):\n",
    "    \"\"\"\n",
    "    Retorna una llista de tokens i les seves entitats.\n",
    "    \"\"\"\n",
    "    return [[(token, entity) for token, pos, entity in sentence] for sentence in sequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(tagged_words, encoding='BIO'):\n",
    "    \"\"\"\n",
    "    Extreu les entitats d'una llista de paraules etiquetades segons l'encoding especificat.\n",
    "    \n",
    "    Arguments:\n",
    "        tagged_words: Una llista de tuples (word, tag), on 'word' és una paraula del text i 'tag' és la seva etiqueta (BIO/BIOW/IO).\n",
    "        encoding: El tipus de codificació utilitzat per les etiquetes ('BIO', 'BIOW', 'IO').\n",
    "        \n",
    "    Retorna:\n",
    "        Una llista de tuples (index_inicial, index_final, etiqueta) que representen les entitats trobades.\n",
    "    \"\"\"\n",
    "    entities = []  # Llista on es guardaran les entitats trobades\n",
    "    current_entity = []  # Guarda les paraules de l'entitat actual\n",
    "    current_type = None  # Tipus de l'entitat actual\n",
    "    current_start_index = None  # Índex d'inici de l'entitat actual\n",
    "\n",
    "    # Recorre cada paraula i la seva etiqueta en la llista de paraules etiquetades\n",
    "    for index, (word, tag) in enumerate(tagged_words):\n",
    "        # Determina el tipus de tag, o 'None' si el tag és 'O'\n",
    "        tag_type = None if tag == 'O' else tag[2:]\n",
    "        # Obté el prefix del tag, o 'None' si és 'O'\n",
    "        tag_prefix = tag[0] if tag != 'O' else None\n",
    "\n",
    "        # Finalitza l'entitat actual si es troba un tag 'O' o si canvia el tipus d'entitat\n",
    "        if tag == 'O' or tag_prefix not in ['B', 'I', 'W'] or (tag_prefix == 'I' and tag_type != current_type):\n",
    "            if current_entity:\n",
    "                entities.append((current_start_index, index - 1, current_type))\n",
    "                current_entity = []\n",
    "                current_type = None\n",
    "        if tag == 'O':\n",
    "            continue\n",
    "\n",
    "        # Gestiona els canvis d'entitat segons l'encoding\n",
    "        if encoding == 'IO' and tag_type != current_type:\n",
    "            if current_entity:\n",
    "                entities.append((current_start_index, index - 1, current_type))\n",
    "            current_entity = [word]\n",
    "            current_start_index = index\n",
    "            current_type = tag_type\n",
    "        elif tag_prefix in ['B', 'W']:\n",
    "            if current_entity:\n",
    "                entities.append((current_start_index, index - 1, current_type))\n",
    "            current_entity = [word]\n",
    "            current_start_index = index\n",
    "            current_type = tag_type\n",
    "            if tag_prefix == 'W':\n",
    "                # Si el tag és 'W-', afegeix immediatament com a entitat de paraula única\n",
    "                entities.append((current_start_index, index, current_type))\n",
    "                current_entity = []\n",
    "                current_type = None\n",
    "        elif tag_prefix == 'I' and current_type == tag_type:\n",
    "            current_entity.append(word)\n",
    "\n",
    "    # Afegeix la darrera entitat si encara hi ha una pendent\n",
    "    if current_entity:\n",
    "        entities.append((current_start_index, index, current_type))\n",
    "\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_entities(true_entities, pred_entities):\n",
    "    \"\"\"\n",
    "    Avaluació de les entitats reconegudes comparant conjunts d'entitats.\n",
    "\n",
    "    Args:\n",
    "    true_entities (list): Llista de tuples representant les entitats reals (start, end, type).\n",
    "    pred_entities (list): Llista de tuples representant les entitats predites (start, end, type).\n",
    "\n",
    "    Returns:\n",
    "    dict: Un diccionari amb les mètriques 'precision', 'recall', i 'f1_score'.\n",
    "    \"\"\"\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    for true_sent, pred_sent in zip(true_entities, pred_entities):\n",
    "        true_set = set(true_sent)\n",
    "        pred_set = set(pred_sent)\n",
    "\n",
    "        true_positives += len(true_set & pred_set)\n",
    "        false_positives += len(pred_set - true_set)\n",
    "        false_negatives += len(true_set - pred_set)\n",
    "\n",
    "    precisio = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    f1_score = 2 * precisio * recall / (precisio + recall) if (precisio + recall) > 0 else 0\n",
    "\n",
    "    # Arrodonim els valors a 3 decimals\n",
    "    precisio = round(precisio, 3)\n",
    "    recall = round(recall, 3)\n",
    "    f1_score = round(f1_score, 3)\n",
    "\n",
    "    return {\n",
    "        'precisio': precisio,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creem un model per predir els postags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crf import *\n",
    "import re\n",
    "import string\n",
    "\n",
    "class FeatureGetterPOS:\n",
    "    def __init__(self):\n",
    "        # Compila un patró regex per detectar dígits\n",
    "        self._pattern = re.compile(r\"\\d\")\n",
    "\n",
    "    def has_digit(self, token):\n",
    "        \"\"\"\n",
    "        Comprova si el token conté algun dígit.\n",
    "        \"\"\"\n",
    "        return any(char.isdigit() for char in token)\n",
    "\n",
    "    def has_punctuation(self, token):\n",
    "        \"\"\"\n",
    "        Comprova si el token conté algun signe de puntuació.\n",
    "        \"\"\"\n",
    "        return any(char in string.punctuation for char in token)\n",
    "\n",
    "    def get_prefix(self, token, n=3):\n",
    "        \"\"\"\n",
    "        Obté el prefix del token.\n",
    "        \"\"\"\n",
    "        return token[:n] if len(token) > n else token\n",
    "\n",
    "    def get_suffix(self, token, n=3):\n",
    "        \"\"\"\n",
    "        Obté el sufix del token.\n",
    "        \"\"\"\n",
    "        return token[-n:] if len(token) > n else token\n",
    "    \n",
    "\n",
    "    def get_features(self, tokens, index):\n",
    "        \"\"\"\n",
    "        Obté les característiques del token.\n",
    "        \"\"\"\n",
    "        token = tokens[index]\n",
    "        features = [\"WORD_\" + token]\n",
    "\n",
    "        if token[0].isupper():\n",
    "            features.append(\"CAPITALIZATION\")\n",
    "\n",
    "        if self.has_digit(token):\n",
    "            features.append(\"HAS_NUM\")\n",
    "\n",
    "        if self.has_punctuation(token):\n",
    "            features.append(\"PUNCTUATION\")\n",
    "            \n",
    "        features.extend([\"SUF_\" + self.get_suffix(token, n) for n in range(1, 4) if len(token) >= n])\n",
    "        features.extend([\"PRE_\" + self.get_prefix(token, n) for n in range(1, 4) if len(token) >= n])\n",
    "        if index > 0:\n",
    "            prev_token = tokens[index - 1]\n",
    "            features.append(\"PREV_WORD_\" + prev_token)\n",
    "        if index < len(tokens) - 1:\n",
    "            next_token = tokens[index + 1]\n",
    "            features.append(\"NEXT_WORD_\" + next_token)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT: La següent cel·la de codi és computacionalment costosa d'executar, és per això que a  continuació carreguem el model ja entrenat prèviament, per tal d'estalviar-nos l'execució d'aquesta cel·la."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Obtenim les dades d'entrenament amb les etiquetes POS\\ntrain_esp_pos = get_token_POS(train_esp)\\n\\n# Creem i configurem el model\\nfeature_getter_pos = FeatureGetterPOS()\\nct_POS = CRFTagger(feature_func=feature_getter_pos.get_features)\\n\\n# Entrenem el model\\nct_POS.train(train_esp_pos, 'model.crf.taggerPOS')\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Obtenim les dades d'entrenament amb les etiquetes POS\n",
    "train_esp_pos = get_token_POS(train_esp)\n",
    "\n",
    "# Creem i configurem el model\n",
    "feature_getter_pos = FeatureGetterPOS()\n",
    "ct_POS = CRFTagger(feature_func=feature_getter_pos.get_features)\n",
    "\n",
    "# Entrenem el model\n",
    "ct_POS.train(train_esp_pos, 'model.crf.taggerPOS')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import CRFTagger\n",
    "\n",
    "feature_getter_pos = FeatureGetterPOS()\n",
    "\n",
    "# Creem una instància del CRFTagger\n",
    "ct_POS = CRFTagger(feature_func=feature_getter_pos.get_features)\n",
    "\n",
    "# Carreguem el model previament entrenat\n",
    "ct_POS.set_model_file('model.crf.taggerPOS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9614033725962005\n"
     ]
    }
   ],
   "source": [
    "# Funció per avaluar el POS tagging\n",
    "def evaluate_POS(true_tags, pred_tags):\n",
    "    \"\"\"\n",
    "    Avaluació del POS tagging comparant seqüències de tags.\n",
    "\n",
    "    Args:\n",
    "    true_tags (list): Llista de llistes amb els tags reals.\n",
    "    pred_tags (list): Llista de llistes amb els tags predits.\n",
    "\n",
    "    Returns:\n",
    "    dict: Un diccionari amb les mètriques 'accuracy'.\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for true_sent, pred_sent in zip(true_tags, pred_tags):\n",
    "        total += len(true_sent)\n",
    "        correct += sum(1 for true_tag, pred_tag in zip(true_sent, pred_sent) if true_tag == pred_tag)\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "\n",
    "    return accuracy\n",
    "  \n",
    "\n",
    "# Prediccions del POS tagging\n",
    "y_pred_POS = ct_POS.tag_sents(get_token(test_esp))\n",
    "y_real_POS = get_token_POS(test_esp)\n",
    "print(evaluate_POS(y_real_POS, y_pred_POS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El model predeix el postags amb una accuracy de 96%. Com que és un bon resultat, l'utilitzarem en les feature functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model per a predir entitats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquesta classe és igual que la anterior, però utilitza el model anterior per predir els postags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "class FeatureGetter:\n",
    "    def __init__(self, ct_POS, use_digit=True, use_punctuation=True, use_prefix=True, use_suffix=True, use_capitalization=True, use_prev_word=True, use_next_word=True):\n",
    "        \"\"\"\n",
    "        Inicialitza el FeatureGetter amb opcions per activar o desactivar certes característiques.\n",
    "        \"\"\"\n",
    "        self.ct_POS = ct_POS\n",
    "        self.use_digit = use_digit\n",
    "        self.use_punctuation = use_punctuation\n",
    "        self.use_prefix = use_prefix\n",
    "        self.use_suffix = use_suffix\n",
    "        self.use_capitalization = use_capitalization\n",
    "        self.use_prev_word = use_prev_word\n",
    "        self.use_next_word = use_next_word\n",
    "        self._pattern = re.compile(r\"\\d\")\n",
    "\n",
    "    def has_digit(self, token):\n",
    "        \"\"\"\n",
    "        Comprova si el token conté algun dígit.\n",
    "        \"\"\"\n",
    "        return any(char.isdigit() for char in token)\n",
    "\n",
    "    def has_punctuation(self, token):\n",
    "        \"\"\"\n",
    "        Comprova si el token conté algun signe de puntuació.\n",
    "        \"\"\"\n",
    "        return any(char in string.punctuation for char in token)\n",
    "\n",
    "    def get_prefix(self, token, n=3):\n",
    "        \"\"\"\n",
    "        Obté el prefix del token.\n",
    "        \"\"\"\n",
    "        return token[:n] if len(token) > n else token\n",
    "\n",
    "    def get_suffix(self, token, n=3):\n",
    "        \"\"\"\n",
    "        Obté el sufix del token.\n",
    "        \"\"\"\n",
    "        return token[-n:] if len(token) > n else token\n",
    "    \n",
    "    def pos_tag(self, token):\n",
    "        \"\"\"\n",
    "        Etiqueta POS del token.\n",
    "        \"\"\"\n",
    "        return self.ct_POS.tag([token])[0][1]\n",
    "\n",
    "\n",
    "    def get_features(self, tokens, index):\n",
    "        \"\"\"\n",
    "        Obté les característiques del token.\n",
    "        \"\"\"\n",
    "        token = tokens[index]\n",
    "        features = [\"WORD_\" + token]\n",
    "\n",
    "        if self.use_capitalization and token[0].isupper():\n",
    "            features.append(\"CAPITALIZATION\")\n",
    "\n",
    "        if self.use_digit and self.has_digit(token):\n",
    "            features.append(\"HAS_NUM\")\n",
    "\n",
    "        if self.use_punctuation and self.has_punctuation(token):\n",
    "            features.append(\"PUNCTUATION\")\n",
    "\n",
    "        if self.use_suffix:\n",
    "            features.extend([\"SUF_\" + self.get_suffix(token, n) for n in range(1, 4) if len(token) >= n])\n",
    "\n",
    "        if self.use_prefix:\n",
    "            features.extend([\"PRE_\" + self.get_prefix(token, n) for n in range(1, 4) if len(token) >= n])\n",
    "\n",
    "        pos_tag = self.pos_tag(token)\n",
    "        features.append(\"POS_\" + pos_tag)\n",
    "        if self.use_prev_word and index > 0:\n",
    "            prev_token = tokens[index - 1]\n",
    "            features.append(\"PREV_WORD_\" + prev_token)\n",
    "\n",
    "        if self.use_next_word and index < len(tokens) - 1:\n",
    "            next_token = tokens[index + 1]\n",
    "            features.append(\"NEXT_WORD_\" + next_token)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from nltk.tag import CRFTagger\n",
    "\n",
    "def grid_search_feature_combinations(train_data, val_data, feature_options):\n",
    "    \"\"\"\n",
    "    Realitza un grid search per trobar la millor combinació de funcions de característiques que maximitza el recall.\n",
    "    \n",
    "    Args:\n",
    "    train_data (list): Dades d'entrenament en format BIO.\n",
    "    val_data (list): Dades de validació en format BIO.\n",
    "    feature_options (dict): Un diccionari amb noms de les característiques com a claus i True/False com a valors per defecte.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: La millor combinació de característiques i el seu valor de recall.\n",
    "    \"\"\"\n",
    "    best_recall = 0\n",
    "    best_combination = None\n",
    "\n",
    "    # Genera totes les combinacions possibles de True/False per a cada característica\n",
    "    keys = feature_options.keys()\n",
    "    train_data = get_token_entity(train_data) \n",
    "    \n",
    "    for values in product([True, False], repeat=len(keys)):\n",
    "        options = dict(zip(keys, values))\n",
    "        feature_getter = FeatureGetter(**options, ct_POS=ct_POS)\n",
    "        ct = CRFTagger(feature_func=feature_getter.get_features)\n",
    "        \n",
    "        # Entrena el model CRFTagger amb les dades d'entrenament\n",
    "        ct.train(train_data, 'model.crf.tagger')\n",
    "\n",
    "        # Prepara les dades de test i obtenir les prediccions\n",
    "        y_real = [extract_entities(sent) for sent in get_token_entity(val_data)]\n",
    "        val_tokens = get_token(val_data)\n",
    "        y_pred = ct.tag_sents(val_tokens)\n",
    "        y_pred_entities = [extract_entities(sent) for sent in y_pred]\n",
    "\n",
    "        # Avaluació de les entitats predites respecte a les reals\n",
    "        metrics = evaluate_entities(y_real, y_pred_entities)\n",
    "        recall = metrics['recall']\n",
    "        # Actualitza la millor combinació si el recall actual és millor\n",
    "        if recall > best_recall:\n",
    "            best_recall = recall\n",
    "            best_combination = options\n",
    "\n",
    "    return best_combination, best_recall\n",
    "\n",
    "# Defineix les opcions de les característiques\n",
    "feature_options = {\n",
    "    'use_prefix': True,\n",
    "    'use_suffix': True,\n",
    "    'use_prev_word': True,\n",
    "    'use_next_word': True\n",
    "}\n",
    "\n",
    "#best_combination, best_recall = grid_search_feature_combinations(train_esp, val_esp, feature_options)\n",
    "#print(\"Best feature combination:\", best_combination)\n",
    "#print(\"Best recall:\", best_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cel·la anterior té un temps d'execució molt elevat, per la qual cosa si es desitja executar, només cal descomentar les últimes tres línies.\n",
    "Per tal d'estalviar tot aquest temps de còmput, els resultats obtinguts són els següents:\n",
    "\n",
    "* Best feature combination: {'use_prefix': True, 'use_suffix': True, 'use_prev_word': True, 'use_next_word': True}\n",
    "* Best recall: 0.711"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com podem veure, la combinació que dona un recall més elevat és amb totes les feature functions actives, per la qual cosa les utilitzarem en el model final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificació BIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quina forma tenen les nostres dades d'entrenament:  [('Melbourne', 'B-LOC'), ('(', 'O'), ('Australia', 'B-LOC'), (')', 'O'), (',', 'O'), ('25', 'O'), ('may', 'O'), ('(', 'O'), ('EFE', 'B-ORG'), (')', 'O'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import CRFTagger\n",
    "import pycrfsuite\n",
    "\n",
    "ct_basic = CRFTagger(feature_func=None)\n",
    "\n",
    "# Train and test sets without the postag\n",
    "train_esp_first = get_token_entity(train_esp)\n",
    "test_esp_first = get_token_entity(test_esp)\n",
    "print(\"Quina forma tenen les nostres dades d'entrenament: \",train_esp_first[0])\n",
    "\n",
    "ct_basic.train(train_esp_first, 'model.crf.tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisio: 0.784\n",
      "Recall: 0.769\n",
      "F1 Score: 0.776\n"
     ]
    }
   ],
   "source": [
    "# Obtenim les dades d'entrenament, que estan en format BIO\n",
    "train_data = get_token_entity(train_esp)  \n",
    "\n",
    "# Creació del model\n",
    "feature_getter = FeatureGetter(ct_POS=ct_POS)\n",
    "ct = CRFTagger(feature_func=feature_getter.get_features)\n",
    "\n",
    "# Entrenament del model\n",
    "ct.train(train_data, 'model.crf.tagger')\n",
    "\n",
    "# Preparació de les dades de prova i predicció\n",
    "y_real = get_token_entity(test_esp)\n",
    "y_pred = ct.tag_sents(get_token(test_esp))\n",
    "\n",
    "# Extraiem les entitats\n",
    "y_real_entities = [extract_entities(sent) for sent in y_real]\n",
    "y_pred_entities = [extract_entities(sent) for sent in y_pred]\n",
    "\n",
    "# Avaluem les entitats\n",
    "metrics = evaluate_entities(y_real_entities, y_pred_entities)\n",
    "print(f'Precisio: {metrics[\"precisio\"]:.3f}')\n",
    "print(f'Recall: {metrics[\"recall\"]:.3f}')\n",
    "print(f'F1 Score: {metrics[\"f1_score\"]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificació IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIO:[('La', 'O'), ('petición', 'O'), ('del', 'O'), ('Abogado', 'B-PER'), ('General', 'I-PER'), ('tiene', 'O'), ('lugar', 'O'), ('después', 'O'), ('de', 'O'), ('que', 'O'), ('un', 'O'), ('juez', 'O'), ('del', 'O'), ('Tribunal', 'B-ORG'), ('Supremo', 'I-ORG'), ('del', 'O'), ('estado', 'O'), ('de', 'O'), ('Victoria', 'B-LOC'), ('(', 'O'), ('Australia', 'B-LOC'), (')', 'O'), ('se', 'O'), ('viera', 'O'), ('forzado', 'O'), ('a', 'O'), ('disolver', 'O'), ('un', 'O'), ('jurado', 'O'), ('popular', 'O'), ('y', 'O'), ('suspender', 'O'), ('el', 'O'), ('proceso', 'O'), ('ante', 'O'), ('el', 'O'), ('argumento', 'O'), ('de', 'O'), ('la', 'O'), ('defensa', 'O'), ('de', 'O'), ('que', 'O'), ('las', 'O'), ('personas', 'O'), ('que', 'O'), ('lo', 'O'), ('componían', 'O'), ('podían', 'O'), ('haber', 'O'), ('obtenido', 'O'), ('información', 'O'), ('sobre', 'O'), ('el', 'O'), ('acusado', 'O'), ('a', 'O'), ('través', 'O'), ('de', 'O'), ('la', 'O'), ('página', 'O'), ('CrimeNet', 'B-MISC'), ('.', 'O')]\n",
      "IO:[('La', 'O'), ('petición', 'O'), ('del', 'O'), ('Abogado', 'I-PER'), ('General', 'I-PER'), ('tiene', 'O'), ('lugar', 'O'), ('después', 'O'), ('de', 'O'), ('que', 'O'), ('un', 'O'), ('juez', 'O'), ('del', 'O'), ('Tribunal', 'I-ORG'), ('Supremo', 'I-ORG'), ('del', 'O'), ('estado', 'O'), ('de', 'O'), ('Victoria', 'I-LOC'), ('(', 'O'), ('Australia', 'I-LOC'), (')', 'O'), ('se', 'O'), ('viera', 'O'), ('forzado', 'O'), ('a', 'O'), ('disolver', 'O'), ('un', 'O'), ('jurado', 'O'), ('popular', 'O'), ('y', 'O'), ('suspender', 'O'), ('el', 'O'), ('proceso', 'O'), ('ante', 'O'), ('el', 'O'), ('argumento', 'O'), ('de', 'O'), ('la', 'O'), ('defensa', 'O'), ('de', 'O'), ('que', 'O'), ('las', 'O'), ('personas', 'O'), ('que', 'O'), ('lo', 'O'), ('componían', 'O'), ('podían', 'O'), ('haber', 'O'), ('obtenido', 'O'), ('información', 'O'), ('sobre', 'O'), ('el', 'O'), ('acusado', 'O'), ('a', 'O'), ('través', 'O'), ('de', 'O'), ('la', 'O'), ('página', 'O'), ('CrimeNet', 'I-MISC'), ('.', 'O')]\n",
      "Entities IO: [(3, 4, 'PER'), (13, 14, 'ORG'), (18, 18, 'LOC'), (20, 20, 'LOC'), (59, 59, 'MISC')]\n"
     ]
    }
   ],
   "source": [
    "def bio_to_io(bio_tagged_sentences):\n",
    "    \"\"\"\n",
    "    Converteix les etiquetes de múltiples frases des de la codificació BIO a IO.\n",
    "    \n",
    "    Arguments:\n",
    "        bio_tagged_sentences: Una llista de llistes de tuples (word, tag) on 'tag' és en codificació BIO.\n",
    "    \n",
    "    Retorna:\n",
    "        Una llista de llistes de tuples (word, io_tag) on 'io_tag' és en codificació IO per cada frase.\n",
    "    \"\"\"\n",
    "    io_tagged_sentences = []\n",
    "    \n",
    "    for sentence in bio_tagged_sentences:\n",
    "        io_tagged_sentence = []\n",
    "        for word, tag in sentence:\n",
    "            if tag.startswith('B-'):\n",
    "                # Canvia B- per I-\n",
    "                io_tagged_sentence.append((word, 'I-' + tag[2:]))\n",
    "            elif tag.startswith('I-'):\n",
    "                io_tagged_sentence.append((word, tag))\n",
    "            else:\n",
    "                # Manté les etiquetes 'O' tal com estan\n",
    "                io_tagged_sentence.append((word, 'O'))\n",
    "        io_tagged_sentences.append(io_tagged_sentence)\n",
    "    \n",
    "    return io_tagged_sentences\n",
    "\n",
    "\n",
    "tagged_words = get_token_entity(train_esp)\n",
    "tagged_words_io = bio_to_io(tagged_words)\n",
    "\n",
    "print(f\"BIO:{tagged_words[3]}\")\n",
    "print(f\"IO:{tagged_words_io[3]}\")\n",
    "\n",
    "entities = extract_entities(tagged_words_io[3], \"IO\")\n",
    "print(f\"Entities IO: {entities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenar el model amb la codificació IO i avaluar els resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precisio': 0.785, 'recall': 0.763, 'f1_score': 0.774}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convertim les dades d'entrenament de format BIO a IO\n",
    "train_data = bio_to_io(get_token_entity(train_esp))\n",
    "\n",
    "# Creem el model i l'entrenem amb les dades d'entrenament\n",
    "feature_getter = FeatureGetter(ct_POS=ct_POS)\n",
    "ct = CRFTagger(feature_func=feature_getter.get_features)\n",
    "ct.train(train_data, 'model.crf.tagger')\n",
    "\n",
    "# Convertim les dades de prova de format BIO a IO i fem les prediccions \n",
    "y_real = bio_to_io(get_token_entity(test_esp))\n",
    "y_pred = bio_to_io(ct.tag_sents(get_token(test_esp)))\n",
    "\n",
    "# Extraiem les entitats reals i predites\n",
    "y_real_entities = [extract_entities(sent, \"IO\") for sent in y_real]\n",
    "y_pred_entities = [extract_entities(sent, \"IO\") for sent in y_pred]\n",
    "\n",
    "# Avaluem les entitats \n",
    "evaluate_entities(y_real_entities, y_pred_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificació BIOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIO:[('Melbourne', 'B-LOC'), ('(', 'O'), ('Australia', 'B-LOC'), (')', 'O'), (',', 'O'), ('25', 'O'), ('may', 'O'), ('(', 'O'), ('EFE', 'B-ORG'), (')', 'O'), ('.', 'O')]\n",
      "BIOW:[('Melbourne', 'W-LOC'), ('(', 'O'), ('Australia', 'W-LOC'), (')', 'O'), (',', 'O'), ('25', 'O'), ('may', 'O'), ('(', 'O'), ('EFE', 'W-ORG'), (')', 'O'), ('.', 'O')]\n",
      "Entities BIOW: [(3, 4, 'PER'), (13, 14, 'ORG'), (18, 18, 'LOC'), (20, 20, 'LOC'), (59, 59, 'MISC')]\n"
     ]
    }
   ],
   "source": [
    "def bio_to_biow(bio_tagged_sentences):\n",
    "    \"\"\"\n",
    "    Converteix les etiquetes de múltiples frases des de la codificació BIO a BIOW.\n",
    "    \n",
    "    Arguments:\n",
    "        bio_tagged_sentences: Una llista de llistes de tuples (word, tag) on 'tag' és en codificació BIO.\n",
    "    \n",
    "    Retorna:\n",
    "        Una llista de llistes de tuples (word, biow_tag) on 'biow_tag' és en codificació BIOW per cada frase.\n",
    "    \"\"\"\n",
    "    biow_tagged_sentences = []\n",
    "    \n",
    "    for sentence in bio_tagged_sentences:\n",
    "        biow_tagged_sentence = []\n",
    "        for i, (word, tag) in enumerate(sentence):\n",
    "            if tag.startswith('B-'):\n",
    "                # Comprovar si l'entitat només té una paraula (W)\n",
    "                if i + 1 < len(sentence) and sentence[i + 1][1].startswith('I-'):\n",
    "                    biow_tagged_sentence.append((word, 'B-' + tag[2:]))\n",
    "                else:\n",
    "                    biow_tagged_sentence.append((word, 'W-' + tag[2:]))\n",
    "            elif tag.startswith('I-'):\n",
    "                biow_tagged_sentence.append((word, tag))\n",
    "            else:\n",
    "                # Manté les etiquetes 'O' tal com estan\n",
    "                biow_tagged_sentence.append((word, 'O'))\n",
    "        biow_tagged_sentences.append(biow_tagged_sentence)\n",
    "    \n",
    "    return biow_tagged_sentences\n",
    "\n",
    "\n",
    "tagged_words = get_token_entity(train_esp)\n",
    "tagged_words_io = bio_to_biow(tagged_words)\n",
    "print(f\"BIO:{tagged_words[0]}\")\n",
    "print(f\"BIOW:{tagged_words_io[0]}\")\n",
    "\n",
    "entities = extract_entities(tagged_words_io[3], \"BIOW\")\n",
    "print(f\"Entities BIOW: {entities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenar el model amb la codificació BIOW i avaluar els resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precisio': 0.792, 'recall': 0.282, 'f1_score': 0.416}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convertim les dades d'entrenament de format BIO a BIOW\n",
    "train_data = bio_to_biow(get_token_entity(train_esp))  \n",
    "\n",
    "# Creem una instància del CRFTagger i l'entrenem \n",
    "feature_getter = FeatureGetter(ct_POS=ct_POS)\n",
    "ct = CRFTagger(feature_func=feature_getter.get_features)\n",
    "ct.train(train_data, 'model.crf.tagger')\n",
    "\n",
    "# Convertim les dades de prova de format BIO a BIOW i fem les prediccions \n",
    "y_real = bio_to_biow(get_token_entity(test_esp))\n",
    "y_pred = bio_to_biow(ct.tag_sents(get_token(test_esp)))\n",
    "\n",
    "# Extraiem les entitats reals i predites\n",
    "y_real_entities = [extract_entities(sent, \"BIOW\") for sent in y_real]\n",
    "y_pred_entities = [extract_entities(sent, \"BIOW\") for sent in y_pred]\n",
    "\n",
    "# Avaluem les entitats\n",
    "evaluate_entities(y_real_entities, y_pred_entities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
