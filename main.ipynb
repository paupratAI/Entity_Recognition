{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pràctica 3 - PLH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realitzada pels alumnes Lluc Furriols i Pau Prat Moreno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import os \n",
    "f = open(\"/dev/null\", \"w\")\n",
    "os.dup2(f.fileno(), 2)\n",
    "f.close()\n",
    "\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importació de llibreries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt', quiet=True) # Tokenitzador\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True) # Etiquetador POS\n",
    "nltk.download('maxent_ne_chunker', quiet=True) # Etiquetador Entitats Anomenades\n",
    "nltk.download('words', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carreguem les dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('conll2002')\n",
    "from nltk.corpus import conll2002\n",
    "\n",
    "train_esp = conll2002.iob_sents('esp.train') # Train, \n",
    "val_esp = conll2002.iob_sents('esp.testa') # Val\n",
    "test_esp = conll2002.iob_sents('esp.testb') # Test\n",
    "\n",
    "train_ned = conll2002.iob_sents('ned.train') # Train\n",
    "val_ned = conll2002.iob_sents('ned.testa') # Val\n",
    "test_ned = conll2002.iob_sents('ned.testb') # Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_esp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token(sequence):\n",
    "    \"\"\"\n",
    "    Retorna una llista de tokens.\n",
    "    \"\"\"\n",
    "    return [[(token) for token, pos, entity in sentence] for sentence in sequence]\n",
    "\n",
    "def get_token_POS(sequence):\n",
    "    \"\"\"\n",
    "    Retorna una llista de tokens i el seu POS tag.\n",
    "    \"\"\"\n",
    "    return [[(token, pos) for token, pos, entity in sentence] for sentence in sequence]\n",
    "\n",
    "def get_token_entity(sequence):\n",
    "    \"\"\"\n",
    "    Retorna una llista de tokens i les seves entitats.\n",
    "    \"\"\"\n",
    "    return [[(token, entity) for token, pos, entity in sentence] for sentence in sequence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First execution with no modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import CRFTagger\n",
    "import pycrfsuite\n",
    "\n",
    "ct = CRFTagger(feature_func=None)\n",
    "\n",
    "# Train and test sets without the postag\n",
    "train_esp_first = get_token_entity(train_esp)\n",
    "test_esp_first = get_token_entity(test_esp)\n",
    "print(\"Quina forma tenen les nostres dades d'entrenament: \",train_esp_first[0])\n",
    "\n",
    "ct.train(train_esp_first, 'model.crf.tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Probar el model en el conjunt de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predir les entitats del conjunt de test\n",
    "y_pred = ct.tag_sents(get_token(test_esp))\n",
    "print(y_pred[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les entitats reals del conjunt de test\n",
    "y_real = get_token_entity(test_esp)\n",
    "print(y_real[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(tagged_words):\n",
    "    entities = []\n",
    "    current_entity = []\n",
    "    current_type = None\n",
    "    current_start_index = None  # Afegim una variable per guardar l'índex d'inici\n",
    "\n",
    "    for index, (word, tag) in enumerate(tagged_words):\n",
    "        if tag.startswith('B-'):  # Començament d'una nova entitat\n",
    "            if current_entity:  # Si hi havia una entitat en construcció, l'afegim abans de començar la nova\n",
    "                entities.append((current_start_index, index - 1, current_type))\n",
    "            current_entity = [word]  # Comencem una nova entitat\n",
    "            current_start_index = index  # Guardem l'índex d'inici de l'entitat actual\n",
    "            current_type = tag[2:]  # Guardem el tipus d'entitat sense el prefix B-\n",
    "        elif tag.startswith('I-') and current_type == tag[2:]:  # Continuació de la mateixa entitat\n",
    "            current_entity.append(word)\n",
    "        else:  # Si no és una continuació de la mateixa entitat o és 'O'\n",
    "            if current_entity:  # Finalitzem l'entitat actual si n'hi ha una\n",
    "                entities.append((current_start_index, index - 1, current_type))\n",
    "                current_entity = []\n",
    "                current_type = None\n",
    "            if tag == 'O':\n",
    "                continue\n",
    "            else:  # Codificació IO o canvi d'entitat amb I-\n",
    "                current_entity = [word]\n",
    "                current_start_index = index\n",
    "                current_type = tag[2:]  # Possible en cas de codificació IO\n",
    "\n",
    "    # Assegurar-se d'afegir l'última entitat si la llista no acaba en 'O'\n",
    "    if current_entity:\n",
    "        entities.append((current_start_index, index, current_type))\n",
    "\n",
    "    return entities\n",
    "\n",
    "\n",
    "def evaluate_entities(y_test, y_pred, print_errors=False):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a named entity recognition model.\n",
    "\n",
    "    This function calculates the precision, recall, and F1 score of the model's predictions. It only evaluates predictions in terms of entities, not individual tokens.\n",
    "    It also optionally prints the sentences where the model made errors.\n",
    "\n",
    "    Parameters:\n",
    "    y_test (list): The true labels for the test data. Each element is a list of tuples, where each tuple contains a token and its true label.\n",
    "    y_pred (list): The predicted labels for the test data. Each element is a list of tuples, where each tuple contains a token and its predicted label.\n",
    "    print_errors (bool, optional): Whether to print the sentences where the model made errors. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    precision (float): The precision of the model's predictions.\n",
    "    recall (float): The recall of the model's predictions.\n",
    "    f1_score (float): The F1 score of the model's predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    total_entities = 0\n",
    "    correct_entities = 0\n",
    "\n",
    "    for sent_test, sent_pred in zip(y_test, y_pred):\n",
    "        true_entities = extract_entities(sent_test)\n",
    "        pred_entities = extract_entities(sent_pred)\n",
    "        \n",
    "        # Comptar entitats reals\n",
    "        entities_sentence = len(true_entities)\n",
    "        # Entitats correctament predites\n",
    "        entities_predicted = len([e for e in true_entities if e in pred_entities])\n",
    "\n",
    "        # Portem el compte de totes les entitats\n",
    "        total_entities += entities_sentence\n",
    "        # Portem el compte de les entitats correctament predites\n",
    "        correct_entities += entities_predicted\n",
    "\n",
    "        if print_errors:\n",
    "            if entities_sentence != entities_predicted:\n",
    "                print('Real sentence:', sent_test)\n",
    "                print('Predicted:', sent_pred)\n",
    "                print('Difference in:', (set(true_entities) - set(pred_entities)))\n",
    "                print()\n",
    "\n",
    "        \n",
    "    if total_entities == 0:\n",
    "        return 0, 0, 0, 0\n",
    "\n",
    "    precision = correct_entities / total_entities\n",
    "    recall = correct_entities / total_entities\n",
    "    f1_score = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    return precision, recall, f1_score\n",
    "\n",
    "\n",
    "precision, recall, f1_score = evaluate_entities(y_real, y_pred, print_errors=False)\n",
    "print(f'Precision: {precision:.6f}')\n",
    "print(f'Recall: {recall:.6f}')\n",
    "print(f'F1 Score: {f1_score:.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_entities(true_entities, pred_entities):\n",
    "    \"\"\"\n",
    "    Avaluació de les entitats reconegudes comparant conjunts d'entitats.\n",
    "\n",
    "    Args:\n",
    "    true_entities (list): Llista de tuples representant les entitats reals (start, end, type).\n",
    "    pred_entities (list): Llista de tuples representant les entitats predites (start, end, type).\n",
    "\n",
    "    Returns:\n",
    "    dict: Un diccionari amb les mètriques 'precision', 'recall', i 'f1_score'.\n",
    "    \"\"\"\n",
    "    true_set = set(true_entities)\n",
    "    pred_set = set(pred_entities)\n",
    "\n",
    "    true_positives = len(true_set & pred_set)\n",
    "    false_positives = len(pred_set - true_set)\n",
    "    false_negatives = len(true_set - pred_set)\n",
    "\n",
    "    precision = true_positives / len(pred_set) if pred_set else 0\n",
    "    recall = true_positives / len(true_set) if true_set else 0\n",
    "    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prescalcular POStags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idea de codi, es nomes per saber que haurem de fer algo per tenir els pos tags precalculats\n",
    "class TagCache:\n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "\n",
    "    def get_tags(self, sentence, tagger):\n",
    "        sentence_hash = hash(tuple(sentence))\n",
    "        if sentence_hash not in self.cache:\n",
    "            self.cache[sentence_hash] = tagger.tag(sentence)\n",
    "        return self.cache[sentence_hash]\n",
    "\n",
    "# Uso de la cache:\n",
    "tag_cache = TagCache()\n",
    "for sentence in some_sentences:\n",
    "    tags = tag_cache.get_tags(sentence, some_tagger)\n",
    "    # Continuar amb el processament\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canviar feature functions i codificacio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "class FeatureGetter:\n",
    "    \"\"\"\n",
    "    Aquesta classe s'utilitza per obtenir diferents característiques d'un token de text.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def has_capitalization(self, token):\n",
    "        return any(char.isupper() for char in token)\n",
    "\n",
    "    def has_digit(self, token):\n",
    "        return any(char.isdigit() for char in token)\n",
    "\n",
    "    def has_punctuation(self, token):\n",
    "        return any(char in string.punctuation for char in token)\n",
    "\n",
    "    def get_prefix(self, token, n=3):\n",
    "        return token[:n] if len(token) > n else token\n",
    "\n",
    "    def get_suffix(self, token, n=3):\n",
    "        return token[-n:] if len(token) > n else token\n",
    "\n",
    "    def all_caps(self, token):\n",
    "        return token.isupper()\n",
    "\n",
    "    def is_capitalized(self, token):\n",
    "        return token[0].isupper()\n",
    "\n",
    "    def get_features(self, tokens, index, add_prefix_suffix=True):\n",
    "        token = tokens[index]\n",
    "        token = str(token)\n",
    "        features = {\n",
    "            'bias': 1.0,\n",
    "            'has_capitalization': self.has_capitalization(token),\n",
    "            'has_digit': self.has_digit(token),\n",
    "            'has_punctuation': self.has_punctuation(token),\n",
    "            'all_caps': self.all_caps(token),\n",
    "            'is_capitalized': self.is_capitalized(token),\n",
    "        }\n",
    "\n",
    "        if add_prefix_suffix:\n",
    "            features['prefix'] = self.get_prefix(token)\n",
    "            features['suffix'] = self.get_suffix(token)\n",
    "\n",
    "        return features\n",
    "\n",
    "# Exemple d'ús:\n",
    "feature_getter = FeatureGetter()\n",
    "tokens = ['Barcelona', 'is', 'beautiful']\n",
    "token_features = feature_getter.get_features(tokens, 0)\n",
    "print(token_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import CRFTagger\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "class FeatureGetter:\n",
    "    def __init__(self):\n",
    "        self._pattern = re.compile(r\"\\d\")\n",
    "\n",
    "    def has_digit(self, token):\n",
    "        return any(char.isdigit() for char in token)\n",
    "\n",
    "    def has_punctuation(self, token):\n",
    "        return any(char in string.punctuation for char in token)\n",
    "\n",
    "    def get_prefix(self, token, n=3):\n",
    "        return token[:n] if len(token) > n else token\n",
    "\n",
    "    def get_suffix(self, token, n=3):\n",
    "        return token[-n:] if len(token) > n else token\n",
    "\n",
    "    def get_features(self, tokens, index):\n",
    "        token = tokens[index]\n",
    "        features = [\"WORD_\" + token]\n",
    "        if token[0].isupper():\n",
    "            features.append(\"CAPITALIZATION\")\n",
    "        if self.has_digit(token):\n",
    "            features.append(\"HAS_NUM\")\n",
    "        if self.has_punctuation(token):\n",
    "            features.append(\"PUNCTUATION\")\n",
    "        features.extend([\"SUF_\" + self.get_suffix(token, n) for n in range(1, 4) if len(token) >= n])\n",
    "        features.extend([\"PRE_\" + self.get_prefix(token, n) for n in range(1, 4) if len(token) >= n])\n",
    "        if index > 0:\n",
    "            prev_token = tokens[index - 1]\n",
    "            features.append(\"PREV_WORD_\" + prev_token)\n",
    "        if index < len(tokens) - 1:\n",
    "            next_token = tokens[index + 1]\n",
    "            features.append(\"NEXT_WORD_\" + next_token)\n",
    "        return features\n",
    "\n",
    "# Creació de dades de prova\n",
    "train_data = [\n",
    "    [('University', 'Noun'), ('is', 'Verb'), ('a', 'Det'), ('Cai', 'gay'), ('good', 'Adj'), ('place', 'Noun')],\n",
    "    [('Dog', 'Noun'), ('eats', 'Verb'), ('meat', 'Noun')]\n",
    "]\n",
    "\n",
    "# Instància del CRFTagger amb la funció de característiques personalitzada\n",
    "feature_getter = FeatureGetter()\n",
    "ct = CRFTagger(feature_func=lambda tokens, index: feature_getter.get_features(tokens, index))\n",
    "\n",
    "# Entrenament del model\n",
    "ct.train(train_data, 'model.crf.tagger')\n",
    "\n",
    "# Previsió en un nou conjunt de dades\n",
    "test_sentences = [['University', 'is', 'good'], ['Cat', 'eats', 'meat'], ['Cai']]\n",
    "tagged_sentences = ct.tag_sents(test_sentences)\n",
    "\n",
    "# Imprimir els resultats etiquetats\n",
    "for sentence in tagged_sentences:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_token_POS(train_esp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "class FeatureGetter:\n",
    "    def __init__(self, use_digit=True, use_punctuation=True, use_capitalization=True, use_suffix_prefix=True):\n",
    "        self.use_digit = use_digit\n",
    "        self.use_punctuation = use_punctuation\n",
    "        self.use_capitalization = use_capitalization\n",
    "        self.use_suffix_prefix = use_suffix_prefix\n",
    "        self._pattern = re.compile(r\"\\d\")\n",
    "        self.punc_cat = {\"Pc\", \"Pd\", \"Ps\", \"Pe\", \"Pi\", \"Pf\", \"Po\"}\n",
    "\n",
    "    def get_features(self, tokens, index):\n",
    "        token = tokens[index]\n",
    "        features = [\"WORD_\" + token]\n",
    "        \n",
    "        if self.use_capitalization and token[0].isupper():\n",
    "            features.append(\"CAPITALIZATION\")\n",
    "        \n",
    "        if self.use_digit and any(char.isdigit() for char in token):\n",
    "            features.append(\"HAS_NUM\")\n",
    "        \n",
    "        if self.use_punctuation and any(unicodedata.category(char) in self.punc_cat for char in token):\n",
    "            features.append(\"PUNCTUATION\")\n",
    "        \n",
    "        if self.use_suffix_prefix:\n",
    "            features.extend([\"SUF_\" + token[-n:] for n in range(1, 4) if len(token) >= n])\n",
    "            features.extend([\"PRE_\" + token[:n] for n in range(1, 4) if len(token) >= n])\n",
    "        \n",
    "        return features\n",
    "train_esp_prepared = get_token_entity(train_esp)\n",
    "val_esp_prepared = get_token_entity(val_esp)\n",
    "train_ned_prepared = get_token_entity(train_ned)\n",
    "val_ned_prepared = get_token_entity(val_ned)\n",
    "\n",
    "# Després utilitza aquestes dades preparades per entrenar i avaluar\n",
    "def train_and_evaluate(feature_config, train_data, validation_data):\n",
    "    fg = FeatureGetter(**feature_config)\n",
    "    ct = CRFTagger(feature_func=lambda tokens, index: fg.get_features(tokens, index))\n",
    "    ct.train(train_data, 'model.crf.tagger')  \n",
    "    return ct.evaluate(validation_data)\n",
    "\n",
    "\n",
    "\n",
    "configurations = [\n",
    "    {'use_digit': True, 'use_punctuation': True, 'use_capitalization': True, 'use_suffix_prefix': True},\n",
    "    {'use_digit': False, 'use_punctuation': True, 'use_capitalization': True, 'use_suffix_prefix': True},\n",
    "    {'use_digit': True, 'use_punctuation': False, 'use_capitalization': True, 'use_suffix_prefix': True},\n",
    "    {'use_digit': True, 'use_punctuation': True, 'use_capitalization': False, 'use_suffix_prefix': True},\n",
    "    {'use_digit': True, 'use_punctuation': True, 'use_capitalization': True, 'use_suffix_prefix': False},\n",
    "]\n",
    "\n",
    "# Prova cada configuració\n",
    "for config in configurations:\n",
    "    accuracy_esp = train_and_evaluate(config, train_esp_prepared, val_esp_prepared)\n",
    "    accuracy_ned = train_and_evaluate(config, train_ned_prepared, val_ned_prepared)\n",
    "    print(f\"Config: {config}, Accuracy ESP: {accuracy_esp}, Accuracy NED: {accuracy_ned}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificació BIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_words = get_token_entity(train_esp)[3]\n",
    "print(tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(tagged_words, encoding='BIO'):\n",
    "    \"\"\"\n",
    "    Extreu les entitats d'una llista de paraules etiquetades segons l'encoding especificat.\n",
    "    \n",
    "    Arguments:\n",
    "        tagged_words: una llista de tuples (word, tag), on 'word' és una paraula del text i 'tag' és la seva etiqueta (BIO/BIOE/BIOW/IO).\n",
    "        encoding: el tipus de codificació utilitzat per les etiquetes ('BIO', 'BIOW', 'BIOE', 'IO').\n",
    "        \n",
    "    Retorna:\n",
    "        Una llista de tuples (start_index, end_index, entity_type) que representen les entitats trobades.\n",
    "        'start_index' i 'end_index' són els índexs on comença i acaba l'entitat en la llista de paraules, i 'entity_type' és el tipus d'entitat.\n",
    "    \"\"\"\n",
    "\n",
    "    entities = []  # Llista on guardarem les entitats trobades\n",
    "    current_entity = []  # Guarda les paraules de l'entitat actual\n",
    "    current_type = None  # Tipus de l'entitat actual\n",
    "    current_start_index = None  # Índex d'inici de l'entitat actual\n",
    "\n",
    "    for index, (word, tag) in enumerate(tagged_words):\n",
    "        tag_type = None if tag == 'O' else tag[2:]\n",
    "\n",
    "        if tag == 'O':\n",
    "            if current_entity:\n",
    "                entities.append((current_start_index, index - 1, current_type))\n",
    "                current_entity = []\n",
    "                current_type = None\n",
    "            continue\n",
    "\n",
    "        if encoding == 'IO':\n",
    "            if tag_type != current_type:\n",
    "                if current_entity:\n",
    "                    entities.append((current_start_index, index - 1, current_type))\n",
    "                current_entity = [word]\n",
    "                current_start_index = index\n",
    "                current_type = tag_type\n",
    "            else:\n",
    "                current_entity.append(word)\n",
    "        else:\n",
    "            tag_prefix = tag[:1]\n",
    "            if tag_prefix in ['B', 'W']:  # Començament d'una nova entitat o entitat de paraula única\n",
    "                if current_entity:\n",
    "                    entities.append((current_start_index, index - 1, current_type))\n",
    "                current_entity = [word]\n",
    "                current_start_index = index\n",
    "                current_type = tag_type\n",
    "                if tag_prefix == 'W':  # Si és una entitat de paraula única, la tanquem immediatament\n",
    "                    entities.append((current_start_index, index, current_type))\n",
    "                    current_entity = []\n",
    "                    current_type = None\n",
    "            elif tag_prefix == 'I' and current_type == tag_type:\n",
    "                current_entity.append(word)\n",
    "            elif encoding == 'BIOE' and tag_prefix == 'E' and current_type == tag_type:\n",
    "                current_entity.append(word)\n",
    "                entities.append((current_start_index, index, current_type))\n",
    "                current_entity = []\n",
    "                current_type = None\n",
    "            else:\n",
    "                if current_entity:\n",
    "                    entities.append((current_start_index, index - 1, current_type))\n",
    "                    current_entity = []\n",
    "                current_type = None\n",
    "\n",
    "    if current_entity:\n",
    "        entities.append((current_start_index, index, current_type))\n",
    "\n",
    "    return entities\n",
    "\n",
    "tagged_words = get_token_entity(train_esp)[3]\n",
    "\n",
    "entities = extract_entities(tagged_words, \"BIO\")\n",
    "print(entities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear un model amb la nova feature function i codificació BIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Quina forma tenen les nostres dades: \",train_esp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def get_clean_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Receives a list and returns the first position of every element, corresponing to the token\n",
    "    \"\"\"\n",
    "    clean = []\n",
    "    for element in sentence: \n",
    "        clean.append(element[0])\n",
    "\n",
    "    return clean\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train_esp_bio = []\n",
    "\n",
    "for sent in train_esp: \n",
    "    i = 0\n",
    "    clean_sentence = get_clean_sentence(sent)\n",
    "    whole_sentence = []\n",
    "    for token, tag, bio in sent:\n",
    "        whole_sentence.append([[token, feature_getter.get_features(clean_sentence, i)], bio])\n",
    "        i = i  + 1\n",
    "    train_esp_bio.append(whole_sentence)\n",
    "\n",
    "print(train_esp_bio[0][0])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"test_esp_bio = []\n",
    "\n",
    "for sent in test_esp: \n",
    "    i = 0\n",
    "    clean_sentence = get_clean_sentence(sent)\n",
    "    whole_sentence = []\n",
    "    for token, tag, bio in sent:\n",
    "        whole_sentence.append([[token, feature_getter.get_features(clean_sentence, i)], bio])\n",
    "        i = i  + 1\n",
    "    test_esp_bio.append(whole_sentence)\n",
    "print(test_esp_bio[0])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ct = CRFTagger(feature_func=feature_getter.get_features)\n",
    "\n",
    "train_esp = get_token_entity(train_esp)\n",
    "\n",
    "ct.train(train_esp, 'model.crf.tagger')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_pred = ct.tag_sents(get_token(test_esp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "precision, recall, f1_score = evaluate_entities(y_real, y_pred, print_errors=False)\n",
    "print(f'Precision: {precision:.6f}')\n",
    "print(f'Recall: {recall:.6f}')\n",
    "print(f'F1 Score: {f1_score:.6f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificació IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bio_to_io(bio_tagged_sentences):\n",
    "    \"\"\"\n",
    "    Converteix les etiquetes de múltiples frases des de la codificació BIO a IO.\n",
    "    \n",
    "    Arguments:\n",
    "        bio_tagged_sentences: Una llista de llistes de tuples (word, tag) on 'tag' és en codificació BIO.\n",
    "    \n",
    "    Retorna:\n",
    "        Una llista de llistes de tuples (word, io_tag) on 'io_tag' és en codificació IO per cada frase.\n",
    "    \"\"\"\n",
    "    io_tagged_sentences = []\n",
    "    \n",
    "    for sentence in bio_tagged_sentences:\n",
    "        io_tagged_sentence = []\n",
    "        for word, tag in sentence:\n",
    "            if tag.startswith('B-'):\n",
    "                # Canvia B- per I-\n",
    "                io_tagged_sentence.append((word, 'I-' + tag[2:]))\n",
    "            elif tag.startswith('I-'):\n",
    "                io_tagged_sentence.append((word, tag))\n",
    "            else:\n",
    "                # Manté les etiquetes 'O' tal com estan\n",
    "                io_tagged_sentence.append((word, 'O'))\n",
    "        io_tagged_sentences.append(io_tagged_sentence)\n",
    "    \n",
    "    return io_tagged_sentences\n",
    "\n",
    "\n",
    "tagged_words = get_token_entity(train_esp)\n",
    "tagged_words_io = bio_to_io(tagged_words)\n",
    "print(f\"BIO:{tagged_words[3]}\")\n",
    "print(f\"IO:{tagged_words_io[3]}\")\n",
    "\n",
    "entities = extract_entities(tagged_words_io[3], \"IO\")\n",
    "print(f\"Entities IO: {entities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from nltk.tag import CRFTagger\n",
    "import pycrfsuite\n",
    "\n",
    "ct = CRFTagger(feature_func=None)\n",
    "#train and test sets without the postag\n",
    "train_esp_bio = get_token_entity(train_esp)\n",
    "test_esp_bio = get_token_entity(test_esp)\n",
    "print(train_esp_bio[0])\n",
    "\n",
    "#y_test is the true labels\n",
    "y_test = [[iob for word, iob in sent] for sent in test_esp_bio]\n",
    "print(y_test[0])\n",
    "\n",
    "#train the model\n",
    "ct.train(train_esp_bio, 'model.crf.tagger')\n",
    "\n",
    "#predict the labels\n",
    "y_pred = ct.tag_sents([[word for word, iob in sent] for sent in test_esp_bio])\n",
    "y_pred = [[iob for word, iob in sent] for sent in y_pred]\n",
    "print(y_pred[0])\n",
    "\n",
    "\n",
    "#show the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Flatten y_test and y_pred\n",
    "y_test_flat = [iob for sent in y_test for iob in sent]\n",
    "y_pred_flat = [iob for sent in y_pred for iob in sent]\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test_flat, y_pred_flat)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()\n",
    "\n",
    "#show the classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test_flat, y_pred_flat))\n",
    "ct.accuracy(test_esp_bio)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def evaluation(true_entities, pred_entities):\n",
    "    \"\"\"\n",
    "    Avalua la predicció de les entitats amb precisió, record i F1-score.\n",
    "    \"\"\"\n",
    "    true_positives = len(set(true_entities) & set(pred_entities))\n",
    "    if true_positives == 0:\n",
    "        return 0, 0, 0\n",
    "    precision = true_positives / len(pred_entities)\n",
    "    recall = true_positives / len(true_entities)\n",
    "    f1_score = 2 * precision * recall / (precision + recall)\n",
    "    return precision, recall, f1_score\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import CRFTagger\n",
    "import pycrfsuite\n",
    "\n",
    "ct = CRFTagger(feature_func=feature_getter.get_features)\n",
    "\n",
    "train_esp_bio = get_token_entity(train_esp)\n",
    "test_esp_bio = get_token_entity(test_esp)\n",
    "\n",
    "ct.train(train_esp_bio, 'model.crf.tagger')\n",
    "\n",
    "# Test the model\n",
    "test_pred = ct.tag_sents(get_token(test_esp))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
