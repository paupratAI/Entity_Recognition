{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pràctica 3 - PLH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realitzada pels alumnes Lluc Furriols i Pau Prat Moreno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os \\nf = open(\"/dev/null\", \"w\")\\nos.dup2(f.fileno(), 2)\\nf.close()\\n\\nimport nltk\\nimport ssl\\n\\ntry:\\n    _create_unverified_https_context = ssl._create_unverified_context\\nexcept AttributeError:\\n    pass\\nelse:\\n    ssl._create_default_https_context = _create_unverified_https_context\\n\\nnltk.download()\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import os \n",
    "f = open(\"/dev/null\", \"w\")\n",
    "os.dup2(f.fileno(), 2)\n",
    "f.close()\n",
    "\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt', quiet=True) # Tokenitzador\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True) # Etiquetador POS\n",
    "nltk.download('maxent_ne_chunker', quiet=True) # Etiquetador Entitats Anomenades\n",
    "nltk.download('words', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass FeatureGetter:\\n    def __init__(ws_cap, ws_prefix,...):\\n        self.ws_cap = ws_cap\\n        self.ws_prefix = ws_prefix\\n        ...\\n    def __call__(self, token, idx):\\n        pass\\ngetter_1 = FeatureGetter(ws_cap=True, ws_prefix=False, ...)\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fet pel profe a la pissarra\n",
    "'''\n",
    "class FeatureGetter:\n",
    "    def __init__(ws_cap, ws_prefix,...):\n",
    "        self.ws_cap = ws_cap\n",
    "        self.ws_prefix = ws_prefix\n",
    "        ...\n",
    "    def __call__(self, token, idx):\n",
    "        pass\n",
    "getter_1 = FeatureGetter(ws_cap=True, ws_prefix=False, ...)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2002 to\n",
      "[nltk_data]     C:\\Users\\llucfurriols\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2002 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('conll2002')\n",
    "from nltk.corpus import conll2002\n",
    "\n",
    "train_esp = conll2002.iob_sents('esp.train') # Train, \n",
    "val_esp = conll2002.iob_sents('esp.testa') # Val\n",
    "test_esp = conll2002.iob_sents('esp.testb') # Test\n",
    "\n",
    "train_ned = conll2002.iob_sents('ned.train') # Train\n",
    "val_ned = conll2002.iob_sents('ned.testa') # Val\n",
    "test_ned = conll2002.iob_sents('ned.testb') # Test\n",
    "\n",
    "# Convertir token postag classe --> postag classe (crec, ha dit 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Melbourne', 'NP', 'B-LOC'), ('(', 'Fpa', 'O'), ('Australia', 'NP', 'B-LOC'), (')', 'Fpt', 'O'), (',', 'Fc', 'O'), ('25', 'Z', 'O'), ('may', 'NC', 'O'), ('(', 'Fpa', 'O'), ('EFE', 'NC', 'B-ORG'), (')', 'Fpt', 'O'), ('.', 'Fp', 'O')], [('-', 'Fg', 'O')], ...]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_esp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def feature_func(tokens, idx):\\n    \\n    Feature function for CRF NER\\n    :param tokens: a list of tuples, each tuple containing (word, pos, iob_tag)\\n    :param idx: the index of the word\\n    \\n    # Inicialitzar el diccionari de característiques\\n    word, pos, iob_tag = tokens[idx]\\n    features = {\\n        'word': word,\\n        'word.lower()': word.lower(),\\n        'is_first': idx == 0,\\n        'is_last': idx == len(tokens) - 1,\\n        'is_capitalized': word[0].upper() == word[0],\\n        'is_all_caps': word.upper() == word,\\n        'is_all_lower': word.lower() == word,\\n        'prefix-1': word[0] if len(word) > 0 else '',\\n        'prefix-2': word[:2] if len(word) > 1 else '',\\n        'prefix-3': word[:3] if len(word) > 2 else '',\\n        'suffix-1': word[-1] if len(word) > 0 else '',\\n        'suffix-2': word[-2:] if len(word) > 1 else '',\\n        'suffix-3': word[-3:] if len(word) > 2 else '',\\n        'prev_word': '' if idx == 0 else tokens[idx - 1][0],\\n        'next_word': '' if idx == len(tokens) - 1 else tokens[idx + 1][0],\\n        'has_hyphen': '-' in word,\\n        'is_numeric': word.isdigit(),\\n        'pos': pos,\\n        'pos_prefix-2': pos[:2],\\n        'pos_prefix-3': pos[:3],\\n        'iob_tag': iob_tag\\n    }\\n    \\n    # Característiques de la paraula anterior\\n    if idx > 0:\\n        prev_word, prev_pos, prev_iob_tag = tokens[idx - 1]\\n        features.update({\\n            'prev_word': prev_word,\\n            'prev_word.lower()': prev_word.lower(),\\n            'prev_word.is_capitalized': prev_word[0].upper() == prev_word[0],\\n            'prev_pos': prev_pos,\\n            'prev_iob_tag': prev_iob_tag\\n        })\\n    else:\\n        features['BOS'] = True  # Indicador de començament de sentència\\n    \\n    # Característiques de la paraula següent\\n    if idx < len(tokens) - 1:\\n        next_word, next_pos, next_iob_tag = tokens[idx + 1]\\n        features.update({\\n            'next_word': next_word,\\n            'next_word.lower()': next_word.lower(),\\n            'next_word.is_capitalized': next_word[0].upper() == next_word[0],\\n            'next_pos': next_pos,\\n            'next_iob_tag': next_iob_tag\\n        })\\n    else:\\n        features['EOS'] = True  # Indicador de final de sentència\\n\\n    return features\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def feature_func(tokens, idx):\n",
    "    \n",
    "    Feature function for CRF NER\n",
    "    :param tokens: a list of tuples, each tuple containing (word, pos, iob_tag)\n",
    "    :param idx: the index of the word\n",
    "    \n",
    "    # Inicialitzar el diccionari de característiques\n",
    "    word, pos, iob_tag = tokens[idx]\n",
    "    features = {\n",
    "        'word': word,\n",
    "        'word.lower()': word.lower(),\n",
    "        'is_first': idx == 0,\n",
    "        'is_last': idx == len(tokens) - 1,\n",
    "        'is_capitalized': word[0].upper() == word[0],\n",
    "        'is_all_caps': word.upper() == word,\n",
    "        'is_all_lower': word.lower() == word,\n",
    "        'prefix-1': word[0] if len(word) > 0 else '',\n",
    "        'prefix-2': word[:2] if len(word) > 1 else '',\n",
    "        'prefix-3': word[:3] if len(word) > 2 else '',\n",
    "        'suffix-1': word[-1] if len(word) > 0 else '',\n",
    "        'suffix-2': word[-2:] if len(word) > 1 else '',\n",
    "        'suffix-3': word[-3:] if len(word) > 2 else '',\n",
    "        'prev_word': '' if idx == 0 else tokens[idx - 1][0],\n",
    "        'next_word': '' if idx == len(tokens) - 1 else tokens[idx + 1][0],\n",
    "        'has_hyphen': '-' in word,\n",
    "        'is_numeric': word.isdigit(),\n",
    "        'pos': pos,\n",
    "        'pos_prefix-2': pos[:2],\n",
    "        'pos_prefix-3': pos[:3],\n",
    "        'iob_tag': iob_tag\n",
    "    }\n",
    "    \n",
    "    # Característiques de la paraula anterior\n",
    "    if idx > 0:\n",
    "        prev_word, prev_pos, prev_iob_tag = tokens[idx - 1]\n",
    "        features.update({\n",
    "            'prev_word': prev_word,\n",
    "            'prev_word.lower()': prev_word.lower(),\n",
    "            'prev_word.is_capitalized': prev_word[0].upper() == prev_word[0],\n",
    "            'prev_pos': prev_pos,\n",
    "            'prev_iob_tag': prev_iob_tag\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True  # Indicador de començament de sentència\n",
    "    \n",
    "    # Característiques de la paraula següent\n",
    "    if idx < len(tokens) - 1:\n",
    "        next_word, next_pos, next_iob_tag = tokens[idx + 1]\n",
    "        features.update({\n",
    "            'next_word': next_word,\n",
    "            'next_word.lower()': next_word.lower(),\n",
    "            'next_word.is_capitalized': next_word[0].upper() == next_word[0],\n",
    "            'next_pos': next_pos,\n",
    "            'next_iob_tag': next_iob_tag\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True  # Indicador de final de sentència\n",
    "\n",
    "    return features\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def word2features(sent, i):\\n    word = sent[i][0]\\n    postag = sent[i][1]\\n\\n    features = {\\n        'bias': 1.0,\\n        'word.lower()': word.lower(),\\n        'word[-3:]': word[-3:],\\n        'word.isupper()': word.isupper(),\\n        'word.istitle()': word.istitle(),\\n        'word.isdigit()': word.isdigit(),\\n        'postag': postag,\\n        'postag[:2]': postag[:2],\\n    }\\n    if i > 0:\\n        word1 = sent[i-1][0]\\n        postag1 = sent[i-1][1]\\n        features.update({\\n            '-1:word.lower()': word1.lower(),\\n            '-1:word.istitle()': word1.istitle(),\\n            '-1:word.isupper()': word1.isupper(),\\n            '-1:postag': postag1,\\n            '-1:postag[:2]': postag1[:2],\\n        })\\n    else:\\n        features['BOS'] = True\\n\\n    if i < len(sent)-1:\\n        word1 = sent[i+1][0]\\n        postag1 = sent[i+1][1]\\n        features.update({\\n            '+1:word.lower()': word1.lower(),\\n            '+1:word.istitle()': word1.istitle(),\\n            '+1:word.isupper()': word1.isupper(),\\n            '+1:postag': postag1,\\n            '+1:postag[:2]': postag1[:2],\\n        })\\n    else:\\n        features['EOS'] = True\\n\\n    return features\\n\\n\\ndef sent2features(sent):\\n    return [word2features(sent, i) for i in range(len(sent))]\\n\\ndef sent2labels(sent):\\n    return [label for token, postag, label in sent]\\n\\ndef sent2tokens(sent):\\n    return [token for token, postag, label in sent]\\n\\nX_train = [sent2features(s) for s in train_esp]\\ny_train = [sent2labels(s) for s in train_esp]\\n\\nX_test = [sent2features(s) for s in test_esp]\\ny_test = [sent2labels(s) for s in test_esp]\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]\n",
    "\n",
    "X_train = [sent2features(s) for s in train_esp]\n",
    "y_train = [sent2labels(s) for s in train_esp]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_esp]\n",
    "y_test = [sent2labels(s) for s in test_esp]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('La', 'DA', 'B-LOC'), ('Coruña', 'NC', 'I-LOC'), (',', 'Fc', 'O'), ('23', 'Z', 'O'), ('may', 'NC', 'O'), ('(', 'Fpa', 'O'), ('EFECOM', 'NP', 'B-ORG'), (')', 'Fpt', 'O'), ('.', 'Fp', 'O')], [('-', 'Fg', 'O')], ...]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_esp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install sklearn-pycrfsuite\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install python-crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9624706498748374"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import CRFTagger\n",
    "import pycrfsuite\n",
    "\n",
    "ct = CRFTagger()\n",
    "#quitar la segunda posicion de todos los elementos de la lista train_esp\n",
    "train_esp = [[(word, iob) for word, pos, iob in sent] for sent in train_esp]\n",
    "test_esp = [[(word, iob) for word, pos, iob in sent] for sent in test_esp]\n",
    "\n",
    "\n",
    "\n",
    "ct.train(train_esp, 'model.crf.tagger')\n",
    "ct.accuracy(test_esp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquesta accuracy no és real --> Exemple que va posar el profe. Sha de fer servir el recall crec\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
