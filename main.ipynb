{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pràctica 3 - PLH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realitzada pels alumnes Lluc Furriols i Pau Prat Moreno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os \\nf = open(\"/dev/null\", \"w\")\\nos.dup2(f.fileno(), 2)\\nf.close()\\n\\nimport nltk\\nimport ssl\\n\\ntry:\\n    _create_unverified_https_context = ssl._create_unverified_context\\nexcept AttributeError:\\n    pass\\nelse:\\n    ssl._create_default_https_context = _create_unverified_https_context\\n\\nnltk.download()\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import os \n",
    "f = open(\"/dev/null\", \"w\")\n",
    "os.dup2(f.fileno(), 2)\n",
    "f.close()\n",
    "\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importació de llibreries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt', quiet=True) # Tokenitzador\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True) # Etiquetador POS\n",
    "nltk.download('maxent_ne_chunker', quiet=True) # Etiquetador Entitats Anomenades\n",
    "nltk.download('words', quiet=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carreguem les dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2002 to\n",
      "[nltk_data]     C:\\Users\\llucfurriols\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2002 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('conll2002')\n",
    "from nltk.corpus import conll2002\n",
    "\n",
    "train_esp = conll2002.iob_sents('esp.train') # Train, \n",
    "val_esp = conll2002.iob_sents('esp.testa') # Val\n",
    "test_esp = conll2002.iob_sents('esp.testb') # Test\n",
    "\n",
    "train_ned = conll2002.iob_sents('ned.train') # Train\n",
    "val_ned = conll2002.iob_sents('ned.testa') # Val\n",
    "test_ned = conll2002.iob_sents('ned.testb') # Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Melbourne', 'NP', 'B-LOC'), ('(', 'Fpa', 'O'), ('Australia', 'NP', 'B-LOC'), (')', 'Fpt', 'O'), (',', 'Fc', 'O'), ('25', 'Z', 'O'), ('may', 'NC', 'O'), ('(', 'Fpa', 'O'), ('EFE', 'NC', 'B-ORG'), (')', 'Fpt', 'O'), ('.', 'Fp', 'O')], [('-', 'Fg', 'O')], ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_esp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token(sequence):\n",
    "    \"\"\"\n",
    "    Retorna una llista de tokens.\n",
    "    \"\"\"\n",
    "    return [[(token) for token, pos, entity in sentence] for sentence in sequence]\n",
    "\n",
    "def get_token_POS(sequence):\n",
    "    \"\"\"\n",
    "    Retorna una llista de tokens i el seu POS tag.\n",
    "    \"\"\"\n",
    "    return [[(token, pos) for token, pos, entity in sentence] for sentence in sequence]\n",
    "\n",
    "def get_token_entity(sequence):\n",
    "    \"\"\"\n",
    "    Retorna una llista de tokens i les seves entitats.\n",
    "    \"\"\"\n",
    "    return [[(token, entity) for token, pos, entity in sentence] for sentence in sequence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First execution with no modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quina forma tenen les nostres dades d'entrenament:  [('Melbourne', 'B-LOC'), ('(', 'O'), ('Australia', 'B-LOC'), (')', 'O'), (',', 'O'), ('25', 'O'), ('may', 'O'), ('(', 'O'), ('EFE', 'B-ORG'), (')', 'O'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import CRFTagger\n",
    "import pycrfsuite\n",
    "\n",
    "ct_basic = CRFTagger(feature_func=None)\n",
    "\n",
    "# Train and test sets without the postag\n",
    "train_esp_first = get_token_entity(train_esp)\n",
    "test_esp_first = get_token_entity(test_esp)\n",
    "print(\"Quina forma tenen les nostres dades d'entrenament: \",train_esp_first[0])\n",
    "\n",
    "ct_basic.train(train_esp_first, 'model.crf.tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Probar el model en el conjunt de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('La', 'B-LOC'), ('Coruña', 'I-LOC'), (',', 'O'), ('23', 'O'), ('may', 'O'), ('(', 'O'), ('EFECOM', 'B-ORG'), (')', 'O'), ('.', 'O')], [('-', 'O')]]\n"
     ]
    }
   ],
   "source": [
    "# Predir les entitats del conjunt de test\n",
    "y_pred = ct_basic.tag_sents(get_token(test_esp))\n",
    "print(y_pred[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('La', 'B-LOC'), ('Coruña', 'I-LOC'), (',', 'O'), ('23', 'O'), ('may', 'O'), ('(', 'O'), ('EFECOM', 'B-ORG'), (')', 'O'), ('.', 'O')], [('-', 'O')]]\n"
     ]
    }
   ],
   "source": [
    "# Les entitats reals del conjunt de test\n",
    "y_real = get_token_entity(test_esp)\n",
    "print(y_real[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 4, 'PER'), (13, 14, 'ORG'), (18, 18, 'LOC'), (20, 20, 'LOC'), (59, 59, 'MISC')]\n"
     ]
    }
   ],
   "source": [
    "def extract_entities(tagged_words, encoding='BIO'):\n",
    "    \"\"\"\n",
    "    Extreu les entitats d'una llista de paraules etiquetades segons l'encoding especificat.\n",
    "    \n",
    "    Arguments:\n",
    "        tagged_words: una llista de tuples (word, tag), on 'word' és una paraula del text i 'tag' és la seva etiqueta (BIO/BIOE/BIOW/IO).\n",
    "        encoding: el tipus de codificació utilitzat per les etiquetes ('BIO', 'BIOW', 'BIOE', 'IO').\n",
    "        \n",
    "    Retorna:\n",
    "        Una llista de tuples (start_index, end_index, entity_type) que representen les entitats trobades.\n",
    "        'start_index' i 'end_index' són els índexs on comença i acaba l'entitat en la llista de paraules, i 'entity_type' és el tipus d'entitat.\n",
    "    \"\"\"\n",
    "\n",
    "    entities = []  # Llista on guardarem les entitats trobades\n",
    "    current_entity = []  # Guarda les paraules de l'entitat actual\n",
    "    current_type = None  # Tipus de l'entitat actual\n",
    "    current_start_index = None  # Índex d'inici de l'entitat actual\n",
    "\n",
    "    for index, (word, tag) in enumerate(tagged_words):\n",
    "        tag_type = None if tag == 'O' else tag[2:]\n",
    "\n",
    "        if tag == 'O':\n",
    "            if current_entity:\n",
    "                entities.append((current_start_index, index - 1, current_type))\n",
    "                current_entity = []\n",
    "                current_type = None\n",
    "            continue\n",
    "\n",
    "        if encoding == 'IO':\n",
    "            if tag_type != current_type:\n",
    "                if current_entity:\n",
    "                    entities.append((current_start_index, index - 1, current_type))\n",
    "                current_entity = [word]\n",
    "                current_start_index = index\n",
    "                current_type = tag_type\n",
    "            else:\n",
    "                current_entity.append(word)\n",
    "        else:\n",
    "            tag_prefix = tag[:1]\n",
    "            if tag_prefix in ['B', 'W']:  # Començament d'una nova entitat o entitat de paraula única\n",
    "                if current_entity:\n",
    "                    entities.append((current_start_index, index - 1, current_type))\n",
    "                current_entity = [word]\n",
    "                current_start_index = index\n",
    "                current_type = tag_type\n",
    "                if tag_prefix == 'W':  # Si és una entitat de paraula única, la tanquem immediatament\n",
    "                    entities.append((current_start_index, index, current_type))\n",
    "                    current_entity = []\n",
    "                    current_type = None\n",
    "            elif tag_prefix == 'I' and current_type == tag_type:\n",
    "                current_entity.append(word)\n",
    "            elif encoding == 'BIOE' and tag_prefix == 'E' and current_type == tag_type:\n",
    "                current_entity.append(word)\n",
    "                entities.append((current_start_index, index, current_type))\n",
    "                current_entity = []\n",
    "                current_type = None\n",
    "            else:\n",
    "                if current_entity:\n",
    "                    entities.append((current_start_index, index - 1, current_type))\n",
    "                    current_entity = []\n",
    "                current_type = None\n",
    "\n",
    "    if current_entity:\n",
    "        entities.append((current_start_index, index, current_type))\n",
    "\n",
    "    return entities\n",
    "\n",
    "# Exemple d'ús\n",
    "tagged_words = get_token_entity(train_esp)[3]\n",
    "\n",
    "entities = extract_entities(tagged_words, \"BIO\")\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_entities(true_entities, pred_entities):\n",
    "    \"\"\"\n",
    "    Avaluació de les entitats reconegudes comparant conjunts d'entitats.\n",
    "\n",
    "    Args:\n",
    "    true_entities (list): Llista de tuples representant les entitats reals (start, end, type).\n",
    "    pred_entities (list): Llista de tuples representant les entitats predites (start, end, type).\n",
    "\n",
    "    Returns:\n",
    "    dict: Un diccionari amb les mètriques 'precision', 'recall', i 'f1_score'.\n",
    "    \"\"\"\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    for true_sent, pred_sent in zip(true_entities, pred_entities):\n",
    "        true_set = set(true_sent)\n",
    "        pred_set = set(pred_sent)\n",
    "\n",
    "        true_positives += len(true_set & pred_set)\n",
    "        false_positives += len(pred_set - true_set)\n",
    "        false_negatives += len(true_set - pred_set)\n",
    "\n",
    "    precisio = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    f1_score = 2 * precisio * recall / (precisio + recall) if (precisio + recall) > 0 else 0\n",
    "\n",
    "\n",
    "    return {\n",
    "        'precision': precisio,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.744405 %\n",
      "Recall: 0.710512 %\n",
      "F1 Score: 0.727064 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "y_real_entities = [extract_entities(sent) for sent in y_real]\n",
    "y_pred_entities = [extract_entities(sent) for sent in y_pred]\n",
    "\n",
    "metrics = evaluate_entities(y_real_entities, y_pred_entities)\n",
    "print(f'Precision: {metrics[\"precision\"]:.6f} %')\n",
    "print(f'Recall: {metrics[\"recall\"]:.6f} %')\n",
    "print(f'F1 Score: {metrics[\"f1_score\"]:.6f} %')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOSTRES MODIFICACIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creem un model per predir els postags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crf import *\n",
    "import re\n",
    "import string\n",
    "\n",
    "class FeatureGetterPOS:\n",
    "    def __init__(self):\n",
    "        self._pattern = re.compile(r\"\\d\")\n",
    "\n",
    "    def has_digit(self, token):\n",
    "        return any(char.isdigit() for char in token)\n",
    "\n",
    "    def has_punctuation(self, token):\n",
    "        return any(char in string.punctuation for char in token)\n",
    "\n",
    "    def get_prefix(self, token, n=3):\n",
    "        return token[:n] if len(token) > n else token\n",
    "\n",
    "    def get_suffix(self, token, n=3):\n",
    "        return token[-n:] if len(token) > n else token\n",
    "    \n",
    "\n",
    "    def get_features(self, tokens, index):\n",
    "        \n",
    "        token = tokens[index]\n",
    "        features = [\"WORD_\" + token]\n",
    "\n",
    "        if token[0].isupper():\n",
    "            features.append(\"CAPITALIZATION\")\n",
    "\n",
    "        if self.has_digit(token):\n",
    "            features.append(\"HAS_NUM\")\n",
    "\n",
    "        if self.has_punctuation(token):\n",
    "            features.append(\"PUNCTUATION\")\n",
    "            \n",
    "        features.extend([\"SUF_\" + self.get_suffix(token, n) for n in range(1, 4) if len(token) >= n])\n",
    "        features.extend([\"PRE_\" + self.get_prefix(token, n) for n in range(1, 4) if len(token) >= n])\n",
    "        if index > 0:\n",
    "            prev_token = tokens[index - 1]\n",
    "            features.append(\"PREV_WORD_\" + prev_token)\n",
    "        if index < len(tokens) - 1:\n",
    "            next_token = tokens[index + 1]\n",
    "            features.append(\"NEXT_WORD_\" + next_token)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_esp_pos = get_token_POS(train_esp)\n",
    "feature_getter_pos = FeatureGetterPOS()\n",
    "\n",
    "ct_POS = CRFTagger(feature_func=feature_getter_pos.get_features)\n",
    "ct_POS.train(train_esp_pos, 'model.crf.taggerPOS')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9614033725962005\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Funció per avaluar el POS tagging\n",
    "def evaluate_POS(true_tags, pred_tags):\n",
    "    \"\"\"\n",
    "    Avaluació del POS tagging comparant seqüències de tags.\n",
    "\n",
    "    Args:\n",
    "    true_tags (list): Llista de llistes amb els tags reals.\n",
    "    pred_tags (list): Llista de llistes amb els tags predits.\n",
    "\n",
    "    Returns:\n",
    "    dict: Un diccionari amb les mètriques 'accuracy'.\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for true_sent, pred_sent in zip(true_tags, pred_tags):\n",
    "        total += len(true_sent)\n",
    "        correct += sum(1 for true_tag, pred_tag in zip(true_sent, pred_sent) if true_tag == pred_tag)\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "\n",
    "    return accuracy\n",
    "  \n",
    "\n",
    "# Prediccions del POS tagging\n",
    "y_pred_POS = ct_POS.tag_sents(get_token(test_esp))\n",
    "y_real_POS = get_token_POS(test_esp)\n",
    "print(evaluate_POS(y_real_POS, y_pred_POS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El model predeix el postags amb una accuracy de 96%. Com que és un bon resultat, l'utilitzarem en les feature functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model per predir entitats:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codificació BIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquesta classe és igual que la anterior, però utilitza el model anterior per predir els postags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import string\n",
    "class FeatureGetter2:\n",
    "    def __init__(self, ct_POS):\n",
    "        self._pattern = re.compile(r\"\\d\")\n",
    "        self.ct_POS = ct_POS\n",
    "\n",
    "    def has_digit(self, token):\n",
    "        return any(char.isdigit() for char in token)\n",
    "\n",
    "    def has_punctuation(self, token):\n",
    "        return any(char in string.punctuation for char in token)\n",
    "\n",
    "    def get_prefix(self, token, n=3):\n",
    "        return token[:n] if len(token) > n else token\n",
    "\n",
    "    def get_suffix(self, token, n=3):\n",
    "        return token[-n:] if len(token) > n else token\n",
    "    \n",
    "    def pos_tag(self, token):\n",
    "        return self.ct_POS.tag([token])[0][1]\n",
    "\n",
    "\n",
    "    def get_features(self, tokens, index):\n",
    "        token = tokens[index]\n",
    "        features = [\"WORD_\" + token]\n",
    "        if token[0].isupper():\n",
    "            features.append(\"CAPITALIZATION\")\n",
    "        if self.has_digit(token):\n",
    "            features.append(\"HAS_NUM\")\n",
    "        if self.has_punctuation(token):\n",
    "            features.append(\"PUNCTUATION\")\n",
    "        features.extend([\"SUF_\" + self.get_suffix(token, n) for n in range(1, 4) if len(token) >= n])\n",
    "        features.extend([\"PRE_\" + self.get_prefix(token, n) for n in range(1, 4) if len(token) >= n])\n",
    "\n",
    "        pos_tag = self.pos_tag(token)\n",
    "        features.append(\"POS_\" + pos_tag)\n",
    "        if index > 0:\n",
    "            prev_token = tokens[index - 1]\n",
    "            features.append(\"PREV_WORD_\" + prev_token)\n",
    "        if index < len(tokens) - 1:\n",
    "            next_token = tokens[index + 1]\n",
    "            features.append(\"NEXT_WORD_\" + next_token)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probem quins resultats dona sense fer cap modificació al model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara veiem quins resultats dona el model amb les nostres modificacions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificació BIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.785878 %\n",
      "Recall: 0.769533 %\n",
      "F1 Score: 0.777620 %\n"
     ]
    }
   ],
   "source": [
    "from crf import *\n",
    "train_data = get_token_entity(train_esp)  \n",
    "\n",
    "# Instància del CRFTagger amb la funció de característiques personalitzada\n",
    "feature_getter = FeatureGetter2(ct_POS=ct_POS)\n",
    "ct = CRFTagger(feature_func=feature_getter.get_features)\n",
    "\n",
    "# Entrenament del model\n",
    "ct.train(train_data, 'model.crf.tagger')\n",
    "\n",
    "y_real = get_token_entity(test_esp)\n",
    "y_pred = ct.tag_sents(get_token(test_esp))\n",
    "\n",
    "y_real_entities = [extract_entities(sent) for sent in y_real]\n",
    "y_pred_entities = [extract_entities(sent) for sent in y_pred]\n",
    "\n",
    "metrics = evaluate_entities(y_real_entities, y_pred_entities)\n",
    "print(f'Precision: {metrics[\"precision\"]:.6f} %')\n",
    "print(f'Recall: {metrics[\"recall\"]:.6f} %')\n",
    "print(f'F1 Score: {metrics[\"f1_score\"]:.6f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificació IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIO:[('La', 'O'), ('petición', 'O'), ('del', 'O'), ('Abogado', 'B-PER'), ('General', 'I-PER'), ('tiene', 'O'), ('lugar', 'O'), ('después', 'O'), ('de', 'O'), ('que', 'O'), ('un', 'O'), ('juez', 'O'), ('del', 'O'), ('Tribunal', 'B-ORG'), ('Supremo', 'I-ORG'), ('del', 'O'), ('estado', 'O'), ('de', 'O'), ('Victoria', 'B-LOC'), ('(', 'O'), ('Australia', 'B-LOC'), (')', 'O'), ('se', 'O'), ('viera', 'O'), ('forzado', 'O'), ('a', 'O'), ('disolver', 'O'), ('un', 'O'), ('jurado', 'O'), ('popular', 'O'), ('y', 'O'), ('suspender', 'O'), ('el', 'O'), ('proceso', 'O'), ('ante', 'O'), ('el', 'O'), ('argumento', 'O'), ('de', 'O'), ('la', 'O'), ('defensa', 'O'), ('de', 'O'), ('que', 'O'), ('las', 'O'), ('personas', 'O'), ('que', 'O'), ('lo', 'O'), ('componían', 'O'), ('podían', 'O'), ('haber', 'O'), ('obtenido', 'O'), ('información', 'O'), ('sobre', 'O'), ('el', 'O'), ('acusado', 'O'), ('a', 'O'), ('través', 'O'), ('de', 'O'), ('la', 'O'), ('página', 'O'), ('CrimeNet', 'B-MISC'), ('.', 'O')]\n",
      "IO:[('La', 'O'), ('petición', 'O'), ('del', 'O'), ('Abogado', 'I-PER'), ('General', 'I-PER'), ('tiene', 'O'), ('lugar', 'O'), ('después', 'O'), ('de', 'O'), ('que', 'O'), ('un', 'O'), ('juez', 'O'), ('del', 'O'), ('Tribunal', 'I-ORG'), ('Supremo', 'I-ORG'), ('del', 'O'), ('estado', 'O'), ('de', 'O'), ('Victoria', 'I-LOC'), ('(', 'O'), ('Australia', 'I-LOC'), (')', 'O'), ('se', 'O'), ('viera', 'O'), ('forzado', 'O'), ('a', 'O'), ('disolver', 'O'), ('un', 'O'), ('jurado', 'O'), ('popular', 'O'), ('y', 'O'), ('suspender', 'O'), ('el', 'O'), ('proceso', 'O'), ('ante', 'O'), ('el', 'O'), ('argumento', 'O'), ('de', 'O'), ('la', 'O'), ('defensa', 'O'), ('de', 'O'), ('que', 'O'), ('las', 'O'), ('personas', 'O'), ('que', 'O'), ('lo', 'O'), ('componían', 'O'), ('podían', 'O'), ('haber', 'O'), ('obtenido', 'O'), ('información', 'O'), ('sobre', 'O'), ('el', 'O'), ('acusado', 'O'), ('a', 'O'), ('través', 'O'), ('de', 'O'), ('la', 'O'), ('página', 'O'), ('CrimeNet', 'I-MISC'), ('.', 'O')]\n",
      "Entities IO: [(3, 4, 'PER'), (13, 14, 'ORG'), (18, 18, 'LOC'), (20, 20, 'LOC'), (59, 59, 'MISC')]\n"
     ]
    }
   ],
   "source": [
    "def bio_to_io(bio_tagged_sentences):\n",
    "    \"\"\"\n",
    "    Converteix les etiquetes de múltiples frases des de la codificació BIO a IO.\n",
    "    \n",
    "    Arguments:\n",
    "        bio_tagged_sentences: Una llista de llistes de tuples (word, tag) on 'tag' és en codificació BIO.\n",
    "    \n",
    "    Retorna:\n",
    "        Una llista de llistes de tuples (word, io_tag) on 'io_tag' és en codificació IO per cada frase.\n",
    "    \"\"\"\n",
    "    io_tagged_sentences = []\n",
    "    \n",
    "    for sentence in bio_tagged_sentences:\n",
    "        io_tagged_sentence = []\n",
    "        for word, tag in sentence:\n",
    "            if tag.startswith('B-'):\n",
    "                # Canvia B- per I-\n",
    "                io_tagged_sentence.append((word, 'I-' + tag[2:]))\n",
    "            elif tag.startswith('I-'):\n",
    "                io_tagged_sentence.append((word, tag))\n",
    "            else:\n",
    "                # Manté les etiquetes 'O' tal com estan\n",
    "                io_tagged_sentence.append((word, 'O'))\n",
    "        io_tagged_sentences.append(io_tagged_sentence)\n",
    "    \n",
    "    return io_tagged_sentences\n",
    "\n",
    "\n",
    "tagged_words = get_token_entity(train_esp)\n",
    "tagged_words_io = bio_to_io(tagged_words)\n",
    "\n",
    "print(f\"BIO:{tagged_words[3]}\")\n",
    "print(f\"IO:{tagged_words_io[3]}\")\n",
    "\n",
    "entities = extract_entities(tagged_words_io[3], \"IO\")\n",
    "print(f\"Entities IO: {entities}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
