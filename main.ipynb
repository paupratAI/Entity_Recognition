{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pràctica 3 - PLH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realitzada pels alumnes Lluc Furriols i Pau Prat Moreno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os \\nf = open(\"/dev/null\", \"w\")\\nos.dup2(f.fileno(), 2)\\nf.close()\\n\\nimport nltk\\nimport ssl\\n\\ntry:\\n    _create_unverified_https_context = ssl._create_unverified_context\\nexcept AttributeError:\\n    pass\\nelse:\\n    ssl._create_default_https_context = _create_unverified_https_context\\n\\nnltk.download()\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import os \n",
    "f = open(\"/dev/null\", \"w\")\n",
    "os.dup2(f.fileno(), 2)\n",
    "f.close()\n",
    "\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt', quiet=True) # Tokenitzador\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True) # Etiquetador POS\n",
    "nltk.download('maxent_ne_chunker', quiet=True) # Etiquetador Entitats Anomenades\n",
    "nltk.download('words', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass FeatureGetter:\\n    def __init__(ws_cap, ws_prefix,...):\\n        self.ws_cap = ws_cap\\n        self.ws_prefix = ws_prefix\\n        ...\\n    def __call__(self, token, idx):\\n        pass\\ngetter_1 = FeatureGetter(ws_cap=True, ws_prefix=False, ...)\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fet pel profe a la pissarra\n",
    "'''\n",
    "class FeatureGetter:\n",
    "    def __init__(ws_cap, ws_prefix,...):\n",
    "        self.ws_cap = ws_cap\n",
    "        self.ws_prefix = ws_prefix\n",
    "        ...\n",
    "    def __call__(self, token, idx):\n",
    "        pass\n",
    "getter_1 = FeatureGetter(ws_cap=True, ws_prefix=False, ...)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2002 to /Users/pau/nltk_data...\n",
      "[nltk_data]   Package conll2002 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('conll2002')\n",
    "from nltk.corpus import conll2002\n",
    "\n",
    "train_esp = conll2002.iob_sents('esp.train') # Train, \n",
    "val_esp = conll2002.iob_sents('esp.testa') # Val\n",
    "test_esp = conll2002.iob_sents('esp.testb') # Test\n",
    "\n",
    "train_ned = conll2002.iob_sents('ned.train') # Train\n",
    "val_ned = conll2002.iob_sents('ned.testa') # Val\n",
    "test_ned = conll2002.iob_sents('ned.testb') # Test\n",
    "\n",
    "# Convertir token postag classe --> postag classe (crec, ha dit 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Melbourne', 'NP', 'B-LOC'), ('(', 'Fpa', 'O'), ('Australia', 'NP', 'B-LOC'), (')', 'Fpt', 'O'), (',', 'Fc', 'O'), ('25', 'Z', 'O'), ('may', 'NC', 'O'), ('(', 'Fpa', 'O'), ('EFE', 'NC', 'B-ORG'), (')', 'Fpt', 'O'), ('.', 'Fp', 'O')], [('-', 'Fg', 'O')], ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_esp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_func(tokens, idx):\n",
    "    \"\"\"\n",
    "    Feature function for CRF NER\n",
    "    :param tokens: a list of tuples, each tuple containing (word, pos, iob_tag)\n",
    "    :param idx: the index of the word\n",
    "    \"\"\"\n",
    "    # Inicialitzar el diccionari de característiques\n",
    "    word, pos, iob_tag = tokens[idx]\n",
    "    features = {\n",
    "        'word': word,\n",
    "        'word.lower()': word.lower(),\n",
    "        'is_first': idx == 0,\n",
    "        'is_last': idx == len(tokens) - 1,\n",
    "        'is_capitalized': word[0].upper() == word[0],\n",
    "        'is_all_caps': word.upper() == word,\n",
    "        'is_all_lower': word.lower() == word,\n",
    "        'prefix-1': word[0] if len(word) > 0 else '',\n",
    "        'prefix-2': word[:2] if len(word) > 1 else '',\n",
    "        'prefix-3': word[:3] if len(word) > 2 else '',\n",
    "        'suffix-1': word[-1] if len(word) > 0 else '',\n",
    "        'suffix-2': word[-2:] if len(word) > 1 else '',\n",
    "        'suffix-3': word[-3:] if len(word) > 2 else '',\n",
    "        'prev_word': '' if idx == 0 else tokens[idx - 1][0],\n",
    "        'next_word': '' if idx == len(tokens) - 1 else tokens[idx + 1][0],\n",
    "        'has_hyphen': '-' in word,\n",
    "        'is_numeric': word.isdigit(),\n",
    "        'pos': pos,\n",
    "        'pos_prefix-2': pos[:2],\n",
    "        'pos_prefix-3': pos[:3],\n",
    "        'iob_tag': iob_tag\n",
    "    }\n",
    "    \n",
    "    # Característiques de la paraula anterior\n",
    "    if idx > 0:\n",
    "        prev_word, prev_pos, prev_iob_tag = tokens[idx - 1]\n",
    "        features.update({\n",
    "            'prev_word': prev_word,\n",
    "            'prev_word.lower()': prev_word.lower(),\n",
    "            'prev_word.is_capitalized': prev_word[0].upper() == prev_word[0],\n",
    "            'prev_pos': prev_pos,\n",
    "            'prev_iob_tag': prev_iob_tag\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True  # Indicador de començament de sentència\n",
    "    \n",
    "    # Característiques de la paraula següent\n",
    "    if idx < len(tokens) - 1:\n",
    "        next_word, next_pos, next_iob_tag = tokens[idx + 1]\n",
    "        features.update({\n",
    "            'next_word': next_word,\n",
    "            'next_word.lower()': next_word.lower(),\n",
    "            'next_word.is_capitalized': next_word[0].upper() == next_word[0],\n",
    "            'next_pos': next_pos,\n",
    "            'next_iob_tag': next_iob_tag\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True  # Indicador de final de sentència\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pycrfsuite' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CRFTagger\n\u001b[0;32m----> 3\u001b[0m ct \u001b[38;5;241m=\u001b[39m \u001b[43mCRFTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_func\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/nltk/tag/crf.py:81\u001b[0m, in \u001b[0;36mCRFTagger.__init__\u001b[0;34m(self, feature_func, verbose, training_opt)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03mInitialize the CRFSuite tagger\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m    :'max_linesearch':  The maximum number of trials for the line search algorithm.\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tagger \u001b[38;5;241m=\u001b[39m \u001b[43mpycrfsuite\u001b[49m\u001b[38;5;241m.\u001b[39mTagger()\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feature_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_features\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pycrfsuite' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.tag import CRFTagger\n",
    "\n",
    "ct = CRFTagger(feature_func=feature_func)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
