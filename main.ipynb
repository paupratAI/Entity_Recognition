{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pràctica 3 - PLH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realitzada pels alumnes Lluc Furriols i Pau Prat Moreno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os \\nf = open(\"/dev/null\", \"w\")\\nos.dup2(f.fileno(), 2)\\nf.close()\\n\\nimport nltk\\nimport ssl\\n\\ntry:\\n    _create_unverified_https_context = ssl._create_unverified_context\\nexcept AttributeError:\\n    pass\\nelse:\\n    ssl._create_default_https_context = _create_unverified_https_context\\n\\nnltk.download()\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import os \n",
    "f = open(\"/dev/null\", \"w\")\n",
    "os.dup2(f.fileno(), 2)\n",
    "f.close()\n",
    "\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importació de llibreries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt', quiet=True) # Tokenitzador\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True) # Etiquetador POS\n",
    "nltk.download('maxent_ne_chunker', quiet=True) # Etiquetador Entitats Anomenades\n",
    "nltk.download('words', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carreguem les dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2002 to /Users/pau/nltk_data...\n",
      "[nltk_data]   Package conll2002 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('conll2002')\n",
    "from nltk.corpus import conll2002\n",
    "\n",
    "train_esp = conll2002.iob_sents('esp.train') # Train, \n",
    "val_esp = conll2002.iob_sents('esp.testa') # Val\n",
    "test_esp = conll2002.iob_sents('esp.testb') # Test\n",
    "\n",
    "train_ned = conll2002.iob_sents('ned.train') # Train\n",
    "val_ned = conll2002.iob_sents('ned.testa') # Val\n",
    "test_ned = conll2002.iob_sents('ned.testb') # Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Melbourne', 'NP', 'B-LOC'), ('(', 'Fpa', 'O'), ('Australia', 'NP', 'B-LOC'), (')', 'Fpt', 'O'), (',', 'Fc', 'O'), ('25', 'Z', 'O'), ('may', 'NC', 'O'), ('(', 'Fpa', 'O'), ('EFE', 'NC', 'B-ORG'), (')', 'Fpt', 'O'), ('.', 'Fp', 'O')], [('-', 'Fg', 'O')], ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_esp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token(sequence):\n",
    "    \"\"\"\n",
    "    Retorna una llista de tokens.\n",
    "    \"\"\"\n",
    "    return [[(token) for token, pos, entity in sentence] for sentence in sequence]\n",
    "\n",
    "def get_token_entity(sequence):\n",
    "    \"\"\"\n",
    "    Retorna una llista de tokens i les seves entitats.\n",
    "    \"\"\"\n",
    "    return [[(token, entity) for token, pos, entity in sentence] for sentence in sequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Melbourne', 'B-LOC'), ('(', 'O'), ('Australia', 'B-LOC'), (')', 'O'), (',', 'O'), ('25', 'O'), ('may', 'O'), ('(', 'O'), ('EFE', 'B-ORG'), (')', 'O'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "print(get_token_entity(train_esp)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bias': 1.0, 'has_capitalization': True, 'has_digit': False, 'has_punctuation': False, 'all_caps': False, 'is_capitalized': True, 'prefix': 'Bar', 'suffix': 'ona'}\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "class FeatureGetter:\n",
    "    \"\"\"\n",
    "    Aquesta classe s'utilitza per obtenir diferents característiques d'un token de text.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def has_capitalization(self, token):\n",
    "        \"\"\"\n",
    "        Comprova si el token conté alguna lletra majúscula.\n",
    "        \"\"\"\n",
    "        return any(char.isupper() for char in token)\n",
    "\n",
    "    def has_digit(self, token):\n",
    "        \"\"\"\n",
    "        Comprova si el token conté algun dígit.\n",
    "        \"\"\"\n",
    "        return any(char.isdigit() for char in token)\n",
    "\n",
    "    def has_punctuation(self, token):\n",
    "        \"\"\"\n",
    "        Comprova si el token conté algun signe de puntuació.\n",
    "        \"\"\"\n",
    "        return any(char in string.punctuation for char in token)\n",
    "\n",
    "    def get_prefix(self, token, n=3):\n",
    "        \"\"\"\n",
    "        Obté el prefix del token amb una longitud de n caràcters.\n",
    "        \"\"\"\n",
    "        return token[:n] if len(token) > n else token\n",
    "\n",
    "    def get_suffix(self, token, n=3):\n",
    "        \"\"\"\n",
    "        Obté el sufix del token amb una longitud de n caràcters.\n",
    "        \"\"\"\n",
    "        return token[-n:] if len(token) > n else token\n",
    "\n",
    "    def all_caps(self, token):\n",
    "        \"\"\"\n",
    "        Comprova si totes les lletres del token són majúscules.\n",
    "        \"\"\"\n",
    "        return token.isupper()\n",
    "\n",
    "    def is_capitalized(self, token):\n",
    "        \"\"\"\n",
    "        Comprova si la primera lletra del token és majúscula.\n",
    "        \"\"\"\n",
    "        return token[0].isupper()\n",
    "\n",
    "    def get_features(self, token, add_prefix_suffix=True):\n",
    "        \"\"\"\n",
    "        Obté un diccionari amb les característiques del token.\n",
    "        \"\"\"\n",
    "        features = {\n",
    "            'bias': 1.0,\n",
    "            'has_capitalization': self.has_capitalization(token),\n",
    "            'has_digit': self.has_digit(token),\n",
    "            'has_punctuation': self.has_punctuation(token),\n",
    "            'all_caps': self.all_caps(token),\n",
    "            'is_capitalized': self.is_capitalized(token),\n",
    "        }\n",
    "\n",
    "        if add_prefix_suffix:\n",
    "            features['prefix'] = self.get_prefix(token)\n",
    "            features['suffix'] = self.get_suffix(token)\n",
    "\n",
    "        return features\n",
    "\n",
    "# Exemple d'ús:\n",
    "feature_getter = FeatureGetter()\n",
    "token_features = feature_getter.get_features('Barcelona')\n",
    "print(token_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificació BIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 4, 'PER'), (13, 14, 'ORG'), (18, 18, 'LOC'), (20, 20, 'LOC'), (59, 59, 'MISC')]\n"
     ]
    }
   ],
   "source": [
    "def extract_entities(tagged_words, encoding='BIO'):\n",
    "    \"\"\"\n",
    "    Extreu les entitats d'una llista de paraules etiquetades segons l'encoding especificat.\n",
    "    \n",
    "    Arguments:\n",
    "        tagged_words: una llista de tuples (word, tag), on 'word' és una paraula del text i 'tag' és la seva etiqueta BIO/BIOE/BIOW.\n",
    "        encoding: el tipus de codificació utilitzat per les etiquetes ('BIO', 'BIOW', 'BIOE').\n",
    "        \n",
    "    Retorna:\n",
    "        Una llista de tuples (start_index, end_index, entity_type) que representen les entitats trobades.\n",
    "        'start_index' i 'end_index' són els índexs on comença i acaba l'entitat en la llista de paraules, i 'entity_type' és el tipus d'entitat.\n",
    "    \"\"\"\n",
    "\n",
    "    entities = []  # Llista on guardarem les entitats trobades\n",
    "    current_entity = []  # Guarda les paraules de l'entitat actual\n",
    "    current_type = None  # Tipus de l'entitat actual\n",
    "    current_start_index = None  # Índex d'inici de l'entitat actual\n",
    "\n",
    "    for index, (word, tag) in enumerate(tagged_words):\n",
    "        if tag == 'O':  # Si el tag és 'O', estem fora d'una entitat\n",
    "            if current_entity:  # Si hi havia una entitat en construcció, l'afegim a la llista\n",
    "                entities.append((current_start_index, index - 1, current_type))\n",
    "                current_entity = []\n",
    "                current_type = None\n",
    "            continue\n",
    "\n",
    "        tag_prefix = tag[:1]  # Obtenim el prefix del tag (B, I, W, E)\n",
    "        tag_type = tag[2:]  # Obtenim el tipus de l'entitat del tag\n",
    "\n",
    "        if encoding in ['BIO', 'BIOW', 'BIOE']:\n",
    "            if tag_prefix in ['B', 'W']:  # Començament d'una nova entitat o entitat de paraula única\n",
    "                if current_entity:\n",
    "                    entities.append((current_start_index, index - 1, current_type))\n",
    "                current_entity = [word]\n",
    "                current_start_index = index\n",
    "                current_type = tag_type\n",
    "                if tag_prefix == 'W':  # Si és una entitat de paraula única, la tanquem immediatament\n",
    "                    entities.append((current_start_index, index, current_type))\n",
    "                    current_entity = []\n",
    "                    current_type = None\n",
    "            elif tag_prefix == 'I' and current_type == tag_type:  # Continuació de l'entitat actual\n",
    "                current_entity.append(word)\n",
    "            elif encoding == 'BIOE' and tag_prefix == 'E' and current_type == tag_type:  # Final d'entitat en BIOE\n",
    "                current_entity.append(word)\n",
    "                entities.append((current_start_index, index, current_type))\n",
    "                current_entity = []\n",
    "                current_type = None\n",
    "            else:  # Si no coincideix el tipus o es troba una etiqueta inesperada, es reseteja l'entitat actual\n",
    "                if current_entity:\n",
    "                    entities.append((current_start_index, index - 1, current_type))\n",
    "                    current_entity = []\n",
    "                current_type = None\n",
    "        else:\n",
    "            raise ValueError(\"Encoding not supported\")  # Si l'encoding no és suportat, llancem una excepció\n",
    "\n",
    "    # Afegir l'última entitat si la llista no acaba en 'O'\n",
    "    if current_entity:\n",
    "        entities.append((current_start_index, index, current_type))\n",
    "\n",
    "    return entities\n",
    "\n",
    "tagged_words = get_token_entity(train_esp)[3]\n",
    "\n",
    "entities = extract_entities(tagged_words)\n",
    "print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('La', 'O'),\n",
       " ('petición', 'O'),\n",
       " ('del', 'O'),\n",
       " ('Abogado', 'B-PER'),\n",
       " ('General', 'I-PER'),\n",
       " ('tiene', 'O'),\n",
       " ('lugar', 'O'),\n",
       " ('después', 'O'),\n",
       " ('de', 'O'),\n",
       " ('que', 'O'),\n",
       " ('un', 'O'),\n",
       " ('juez', 'O'),\n",
       " ('del', 'O'),\n",
       " ('Tribunal', 'B-ORG'),\n",
       " ('Supremo', 'I-ORG'),\n",
       " ('del', 'O'),\n",
       " ('estado', 'O'),\n",
       " ('de', 'O'),\n",
       " ('Victoria', 'B-LOC'),\n",
       " ('(', 'O'),\n",
       " ('Australia', 'B-LOC'),\n",
       " (')', 'O'),\n",
       " ('se', 'O'),\n",
       " ('viera', 'O'),\n",
       " ('forzado', 'O'),\n",
       " ('a', 'O'),\n",
       " ('disolver', 'O'),\n",
       " ('un', 'O'),\n",
       " ('jurado', 'O'),\n",
       " ('popular', 'O'),\n",
       " ('y', 'O'),\n",
       " ('suspender', 'O'),\n",
       " ('el', 'O'),\n",
       " ('proceso', 'O'),\n",
       " ('ante', 'O'),\n",
       " ('el', 'O'),\n",
       " ('argumento', 'O'),\n",
       " ('de', 'O'),\n",
       " ('la', 'O'),\n",
       " ('defensa', 'O'),\n",
       " ('de', 'O'),\n",
       " ('que', 'O'),\n",
       " ('las', 'O'),\n",
       " ('personas', 'O'),\n",
       " ('que', 'O'),\n",
       " ('lo', 'O'),\n",
       " ('componían', 'O'),\n",
       " ('podían', 'O'),\n",
       " ('haber', 'O'),\n",
       " ('obtenido', 'O'),\n",
       " ('información', 'O'),\n",
       " ('sobre', 'O'),\n",
       " ('el', 'O'),\n",
       " ('acusado', 'O'),\n",
       " ('a', 'O'),\n",
       " ('través', 'O'),\n",
       " ('de', 'O'),\n",
       " ('la', 'O'),\n",
       " ('página', 'O'),\n",
       " ('CrimeNet', 'B-MISC'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_token_entity(train_esp)[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificació IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from nltk.tag import CRFTagger\\nimport pycrfsuite\\n\\nct = CRFTagger(feature_func=None)\\n#train and test sets without the postag\\ntrain_esp_bio = get_token_entity(train_esp)\\ntest_esp_bio = get_token_entity(test_esp)\\nprint(train_esp_bio[0])\\n\\n#y_test is the true labels\\ny_test = [[iob for word, iob in sent] for sent in test_esp_bio]\\nprint(y_test[0])\\n\\n#train the model\\nct.train(train_esp_bio, 'model.crf.tagger')\\n\\n#predict the labels\\ny_pred = ct.tag_sents([[word for word, iob in sent] for sent in test_esp_bio])\\ny_pred = [[iob for word, iob in sent] for sent in y_pred]\\nprint(y_pred[0])\\n\\n\\n#show the confusion matrix\\nfrom sklearn.metrics import confusion_matrix\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n# Flatten y_test and y_pred\\ny_test_flat = [iob for sent in y_test for iob in sent]\\ny_pred_flat = [iob for sent in y_pred for iob in sent]\\n\\n# Generate confusion matrix\\ncm = confusion_matrix(y_test_flat, y_pred_flat)\\n\\n# Visualize confusion matrix\\nplt.figure(figsize=(10,7))\\nsns.heatmap(cm, annot=True, fmt='d')\\nplt.xlabel('Predicted')\\nplt.ylabel('Truth')\\nplt.show()\\n\\n#show the classification report\\nfrom sklearn.metrics import classification_report\\nprint(classification_report(y_test_flat, y_pred_flat))\\nct.accuracy(test_esp_bio)\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from nltk.tag import CRFTagger\n",
    "import pycrfsuite\n",
    "\n",
    "ct = CRFTagger(feature_func=None)\n",
    "#train and test sets without the postag\n",
    "train_esp_bio = get_token_entity(train_esp)\n",
    "test_esp_bio = get_token_entity(test_esp)\n",
    "print(train_esp_bio[0])\n",
    "\n",
    "#y_test is the true labels\n",
    "y_test = [[iob for word, iob in sent] for sent in test_esp_bio]\n",
    "print(y_test[0])\n",
    "\n",
    "#train the model\n",
    "ct.train(train_esp_bio, 'model.crf.tagger')\n",
    "\n",
    "#predict the labels\n",
    "y_pred = ct.tag_sents([[word for word, iob in sent] for sent in test_esp_bio])\n",
    "y_pred = [[iob for word, iob in sent] for sent in y_pred]\n",
    "print(y_pred[0])\n",
    "\n",
    "\n",
    "#show the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Flatten y_test and y_pred\n",
    "y_test_flat = [iob for sent in y_test for iob in sent]\n",
    "y_pred_flat = [iob for sent in y_pred for iob in sent]\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test_flat, y_pred_flat)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()\n",
    "\n",
    "#show the classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test_flat, y_pred_flat))\n",
    "ct.accuracy(test_esp_bio)'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
